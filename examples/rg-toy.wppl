// iterate(5, 0, function(x) { x + 1 });
// =>
// [1,2,3,4,5]
var iterate = function(n, init, f) {
  if (n === 0) {
    return [];
  } else {
    var val = f(init);
    return [val].concat(iterate(n - 1, val, f));
  }
};

var pp = function(x) { console.log(JSON.stringify(x, null, 2)); };

var isSingleton = function(t) { return t.dims.length === 1 && t.dims[0] === 1; };

// Extract a simple name => value mapping from the webppl params
// object.
var extractParams = function(params) {
  return _.object(map(function(pair) {
    assert.ok(pair[1].length === 1, 'Cannot handle multiple parameters per name.');
    var name = pair[0];
    var val = ad.value(pair[1][0]);
    return [name, isSingleton(val) ? ad.tensor.get(val, 0) : val];
  }, _.pairs(params)));
};

// Pretty print parameters.
var ppp = function(params) { pp(extractParams(params)); };

var bindParams = function(params, f) {
  return function() {
    var args = arguments;
    sample(Infer({method: 'forward', guide: true, params: params}, function() {
      return apply(f, args);
    }));
  };
};

// Make a dictionary type object where look up uses function
// application rather than member expression.
var dict = function(obj) {
  return function(key) {
    return obj[key];
  };
};

// state is threaded along the program and contains:
// i      | the index of the current step
// ctx    | the guide hidden state/context
// val    | sampled value
// p      | parameter of guided Bernoulli
var initialState = {i: 0, ctx: 0, val: undefined, p: undefined};

// This captures the general structure of the recurrent guide for a
// single coin flip. This is the core of the model.
var makeStepFn = function(guide) {
  return function(state) {
    var predict = guide('predict'), update = guide('update'), embed = guide('embed');

    var p = predict(state);

    var val = sample(Bernoulli({p: .5}), {
      guide: Bernoulli({p: p})
    });

    var ctx = update(state, embed(val));

    return {
      i: state.i + 1,
      ctx: ctx,
      val: val,
      p: ad.value(p)
    };
  };
};

// A guide architecture is specified by a function like this. (Written
// in a funny way since we can't call wppl functions using member
// expressions.)
var blah = dict({
  predict: function() {
  },
  update: function() {
  },
  embed: function() {
  }
});

// A mean-field guide.
var mf = dict({
  predict: function(state) {
    return Math.sigmoid(param({name: 'p' + state.i}));
  },
  update: constF(0),
  embed: idF
});

// A simple guide to capture the correlation.

var predictLinSigPerChoice = function(state) {
  // The predict function is a linear function of the context,
  // appropriately squished, with per choice parameters.
  var i = state.i;
  var w = param({name: 'w' + i});
  var b = param({name: 'b' + i});
  return Math.sigmoid(w * state.ctx + b);
};

// Observation: As well as capturing the correlation, this also
// appears to approximate the intermediate choices better then mean
// field.

var simple = dict({
  predict: predictLinSigPerChoice,
  update: function(state, v) {
    // At the first step, stick the (embedded) value into the
    // context. (We know this is sufficient for this problem.)
    return state.i === 0 ? v : state.ctx;
  },
  embed: function(val) {
    return val ? 1 : 0; // Q: How does different embeddings (e.g. .5,-.5) affect optimization?
  }
});

// Now consider allowing the guide the flexibility to alter the
// context after each choice. We don't yet allow sampled values to be
// incorporated into the state. Note that the previous guide is just a
// special case of this, where w=1 and b=0.

// Observation: This change introduces more sensitivity to the choice
// of learning rate. The previous guide usually optimizes OK with a
// step size of 0.1, this usually doesn't.

// Observation: I've not seen this learn that identity for updates.
// Instead, w is always no zero causing the magnitude of the context
// to increase during execution. When w is negative, this sets up an
// oscillation in the context.

var simple2 = dict({
  predict: predictLinSigPerChoice,
  update: function(state, v) {
    var w = param({name: 'w_up'});
    var b = param({name: 'b_up'});
    return state.i === 0 ? v : w * state.ctx + b;
  },
  embed: function(val) {
    return val ? 1 : 0;
  }
});

// This specifies the version of the guide we run below.
var guide = simple2;

var model = function() {
  var steps = iterate(5, initialState, makeStepFn(guide));
  // Condition
  // Exact posterior:
  // [false,true]  : 0.4762870634112165
  // [true,false]  : 0.4762870634112165
  // [false,false] : 0.023712936588783384
  // [true,true]   : 0.023712936588783384
  var firstVal = first(steps).val;
  var lastVal = last(steps).val;
  factor(firstVal !== lastVal ? 0 : -3);
  return [firstVal, lastVal];
};

var params = Optimize(model, {steps: 20000, optMethod: {adam: {stepSize: 0.01}}});
ppp(params);

var m = Infer({method: 'forward', params: params, guide: true, samples: 1000}, model);
display(m.print());

var fakeit = bindParams(params, function(val) {
  var predict = guide('predict'), update = guide('update'), embed = guide('embed');
  // Pretend we sampled val at the first choice, and then run the
  // guide from there.
  var ctx = update(initialState, embed(val));
  var state = {i: 1, ctx: ctx, val: val, p: null};
  return [state].concat(iterate(4, state, makeStepFn(guide)));
});

var formatStep = function(step) {
  var padNonNeg = function(s) { return s[0] === '-' ? s : ' '.concat(s); };
  var pad = function(n, s) { return s.length >= n ? s : s.concat(' '.repeat(n - s.length)); };
  return [
    'i ', step.i,
    ' | p ', step.p === null ? pad(8, '*') : step.p.toFixed(6),
    ' | val ', pad(5, step.val.toString()),
    ' | ctx ', padNonNeg(ad.value(step.ctx).toFixed(6))
  ].join('');
};

var ppSteps = function(steps) { pp(map(formatStep, steps)); };

ppSteps(fakeit(true));
ppSteps(fakeit(false));
