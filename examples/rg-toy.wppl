// iterate(5, 0, function(x) { x + 1 });
// =>
// [1,2,3,4,5]
var iterate = function(n, init, f) {
  if (n === 0) {
    return [];
  } else {
    var val = f(init);
    return [val].concat(iterate(n - 1, val, f));
  }
};

var pp = function(x) { console.log(JSON.stringify(x, null, 2)); };

var isSingleton = function(t) { return t.dims.length === 1 && t.dims[0] === 1; };

// Extract a simple name => value mapping from the webppl params
// object.
var extractParams = function(params) {
  return _.object(map(function(pair) {
    assert.ok(pair[1].length === 1, 'Cannot handle multiple parameters per name.');
    var name = pair[0];
    var val = ad.value(pair[1][0]);
    return [name, isSingleton(val) ? ad.tensor.get(val, 0) : val];
  }, _.pairs(params)));
};

// Pretty print parameters.
var ppp = function(params) { pp(extractParams(params)); };

var bindParams = function(params, f) {
  return function() {
    var args = arguments;
    sample(Infer({method: 'forward', guide: true, params: params}, function() {
      return ad.valueRec(apply(f, args));
    }));
  };
};

// Make a dictionary type object where look up uses function
// application rather than member expression.
var dict = function(obj) {
  return function(key) {
    return obj[key];
  };
};

var formatStep = function(step) {
  var padNonNeg = function(s) { return s[0] === '-' ? s : ' '.concat(s); };
  var pad = function(n, s) { return s.length >= n ? s : s.concat(' '.repeat(n - s.length)); };
  return [
    'i ', step.i,
    ' | p ', step.p === null ? pad(8, '*') : step.p.toFixed(6),
    ' | val ', pad(5, step.val.toString()),
    ' | ctx ', padNonNeg(step.ctx.toFixed(6))
  ].join('');
};

var ppSteps = function(steps) { pp(map(formatStep, steps)); };

// state is threaded along the program and contains:
// i      | the index of the current step
// ctx    | the guide hidden state/context
// val    | sampled value
// p      | parameter of guided Bernoulli
var initialState = {i: 0, ctx: 0, val: undefined, p: undefined};

// This captures the general structure of the recurrent guide for a
// single coin flip. This is the core of the model.
var makeStepFn = function(guide) {
  return function(state) {
    var predict = guide('predict'), update = guide('update'), embed = guide('embed');

    var p = predict(state);

    var val = sample(Bernoulli({p: .5}), {
      guide: Bernoulli({p: p})
    });

    var ctx = update(state, embed(val));

    return {
      i: state.i + 1,
      ctx: ctx,
      val: val,
      p: p
    };
  };
};

// A guide architecture is specified by a function like this. (Written
// in a funny way since we can't call wppl functions using member
// expressions.)
var blah = dict({
  predict: function() {
  },
  update: function() {
  },
  embed: function() {
  }
});

// A mean-field guide.
var mf = dict({
  predict: function(state) {
    return Math.sigmoid(param({name: 'p' + state.i}));
  },
  update: constF(0),
  embed: idF
});

// A simple guide to capture the correlation.

var predictLinSigPerChoice = function(state) {
  // The predict function is a linear function of the context,
  // appropriately squished, with per choice parameters.
  var i = state.i;
  var w = param({name: 'w' + i});
  var b = param({name: 'b' + i});
  return Math.sigmoid(w * state.ctx + b);
};

// Observation: As well as capturing the correlation, this also
// appears to approximate the intermediate choices better then mean
// field.

var simple = dict({
  predict: predictLinSigPerChoice,
  update: function(state, v) {
    // At the first step, stick the (embedded) value into the
    // context. (We know this is sufficient for this problem.)
    return state.i === 0 ? v : state.ctx;
  },
  embed: function(val) {
    return val ? 1 : 0; // Q: How does different embeddings (e.g. .5,-.5) affect optimization?
  }
});

// Now consider allowing the guide the flexibility to alter the
// context after each choice. We don't yet allow sampled values to be
// incorporated into the state. Note that the previous guide is just a
// special case of this, where w=1 and b=0.

var updateLin = function(state, v) {
  var w = param({name: 'w_up'});
  var b = param({name: 'b_up'});
  return state.i === 0 ? v : w * state.ctx + b;
};

// Observation: This change introduces more sensitivity to the choice
// of learning rate. The previous guide usually optimizes OK with a
// step size of 0.1, this usually doesn't.

// Observation: I've not seen this learn that identity for updates.
// Instead, w is always no zero causing the magnitude of the context
// to increase during execution. When w is negative, this sets up an
// oscillation in the context.

var simple2 = dict({
  predict: predictLinSigPerChoice,
  update: updateLin,
  embed: function(val) {
    return val ? 1 : 0;
  }
});

// Now we share weights across choices in the predict function.
// Previously, the predict for each intermediate choice could ignore
// the guide (with w=0) and predict 0.5.

// Note that just adding weight sharing (as in predictLinSig below)
// does not allow posterior to be captured accurately.

var predictLinSig = function(state) {
  var i = state.i;
  var w = param({name: 'w'});
  var b = param({name: 'b'});
  return Math.sigmoid(w * state.ctx + b);
};

// One way you can imagine fixing this is to try to allow the
// intermediate choices to ignore the state.

// (Another, not explored here, might be to have a non-linear update
// function that can approximately count. This might be able to keep
// the state roughly constat for the intermediate choices, and then
// modify it before the final choice. This would still benefit from a
// richer predict function though, as you'd want to map the two
// contexts that encode the inital choice to the same prediction.)

// The question then is, what function will allow the context to be
// ignored base on the current position?

// One option is to have a little 2 layer MLP that takes as input the
// context and an embedding of the current position.

var predictMlp = function(state, nonlinearfn) {
  assert.ok(nonlinearfn, 'Non-linear function not given.');
  var i = state.i;
  var ctx = state.ctx;
  var pos = param({name: 'pos'+i});
  var h1 = nonlinearfn(ctx * param({name: 'wp11'}) + param({name: 'wp12'}) * pos + param({name: 'bp1'}));
  var h2 = nonlinearfn(ctx * param({name: 'wp21'}) + param({name: 'wp22'}) * pos + param({name: 'bp2'}));
  return Math.sigmoid(param({name: 'vp1'}) * h1 + param({name: 'vp2'}) * h2 + param({name: 'cp1'}));
};

var sigmoid = function(x) { return Math.sigmoid(x); };
var tanh = function(x) { return Math.tanh(x); };

var predictMlpSigmoid = function(state) { return predictMlp(state, sigmoid); };
var predictMlpTanh = function(state) { return predictMlp(state, tanh); };

// (Note that the shallow version of this, sigmoid(pos + w * ctx + b)
// does not work, since pos and ctx can't interact in the required
// way. Which is kinda multiplicatively?)

// This works nicely, though I'm not entirely clear *what* the MLP
// ends up implementing here. My guess is that rather than learning to
// *ignore* the context, it learns to map both of the two contexts it
// encounters to 0.5.

// Observation: This is interesting because it feels like we'd really
// rather learn to ignore the context here. I'm guessing that with the
// MLP all parts of the guide are interconnected in complicated ways
// that (1) may be tricky to learn and (2) might not generalize well.
// If the guide for the intermediate choices could become disconnected
// from the context then we can have independent guide fragments for
// those, and the context part only has to handle capturing the
// correlation.

// With that in mind, we can try using a gate to make the option to
// ignore the context within easy reach of optimization. This is a
// very simple gate: if open we predict from the context, if its
// closed we predict 0.5. This is fine for this model, but would need
// to be generalized for other models.

// Q: How can we keep the guide flexible if it ignores the context. We
// don't really want to have lots of mean field parameters available
// just in case. Perhaps the generalization of this to vector valued
// contexts is to allow predict to ignore *parts* of the context.
// Parts of the context might carry information about previous choices
// which can be ignored, while the part of the context encoding the
// current datum is used for a prediction. Having a way that the
// predict function can avoid having to handle variation in irrelevant
// parts of the context might still be useful?

// Q: How does optimization progress compare across mlp vs. gated?

// Observation: A guide optimized with the gated version tends to use
// less extreme values for the context compared to the MLP version.
// The final value of the context might have magnitude ~10 rather than
// ~200.

var predictBasicGate = function(state) {
  var i = state.i;
  var ctx = state.ctx;
  // Per choice gate.
  var pos = param({name: 'pos'+i});
  var gate = Math.sigmoid(pos);
  // Predict from context.
  var w = param({name: 'w'});
  var b = param({name: 'b'});
  return Math.sigmoid(gate * (w * state.ctx + b));
};

var simple3 = dict({
  predict: predictBasicGate,
  //predict: predictMlpSigmoid,
  update: updateLin,
  embed: function(val) {
    return val ? 1 : 0;
  }
});


// Here we add flexibility to the way in which sampled values are
// added to the context, removing the current strategy of adding the
// important random choice explicitly.

// How should this work? The obvious first thing to try is to set
// things up such that it's clear the guide can mimic the current
// hand-coded thing. To that end, we have a per choice gate that
// decides whether to pass on the current context or use the sampled
// value.

var updateWithVal = function(state, v) {
  // v is the embedded value.
  var i = state.i;
  var g = Math.sigmoid(param({name: 'g_up' + i}));
  return g * v + (1 - g) * state.ctx; // note, always in [0,1] with embed of t=>1, f=>0.
};

// This works reliably when paired with predictBasicGate.

// Observation: When paired with predictMlpSigmoid, this almost always
// fails to find the optimal solution. Interestingly predictMlp
// continues to fail when a suitable strategy for setting the context
// (to either 0 or 1) is hand-coded. Note that updateWithVal only ever
// sets the state to between zero and one because of the embedding and
// form of the update. It appears that the success of predictMlp
// strongly depends on update increasing the magnitude of the context
// at each step. i.e. As implemented by updateLin.

// Observation: Switching to predictMlpTanh (rather than
// predictMlpSigmoid) appears to make it work *much* more reliably.
// (I've only seen sigmoid work twice, and I've only seen tanh *not*
// work once.)

var simple4 = dict({
  //predict: predictMlpSigmoid,
  //predict: predictMlpTanh,
  predict: predictBasicGate,
  update: updateWithVal,
  // update: function(state, v) {
  //  return state.i === 0 ? v : state.ctx;
  // },
  embed: function(val) {
    return val ? 1 : 0;
  }
});

// Q: One question to ask is how long can the program get before we
// stop been able to optimize this simple guide? If I increase the
// length to 10, then optimization can still find a good solution, but
// I had to run for ~100K steps.

// Observation: If I just add the previous context to the value
// returned by updateWithVal then optimization can find good solutions
// to long sequences much more quickly. This is a little unsatisfying
// as the context gets very large, but is perhaps evidence that skip
// connections can help here.



// This is (an approximation to) the strategy current implemented by
// daipp.

// Observation: This doesn't seem to do much unless I switch the
// hidden layer of predictMlp from sigmoid to tanh. (I made the same
// observation in another setting above. Though here, tanh is also
// fairly unreliably.) Changing the embedding from 0,1 to -.5,.5 looks
// like it might help a little.

// TODO: It would be good to understand exactly why sigmoid is so
// tricky to optimize here. (Or in the case above.)


// Observation: The way this appears to solve the problem is by
// setting up an oscillation in the context.

// TODO: Try some variations of this model to see whether these guides
// can find solutions to those too. One simple thing to do is to have
// both the last and second last choices (anti)correlated with the
// first choice. At first glance, this appears to defeat the current
// daipp guide. (Even with tanh.) The version of this where the one of
// the last two is correlated and the other is anti-correlated defeats
// simple4.

// TODO: Another thing to try could be to have the last choice be a
// (noisy/soft) function of the first two choices. So instead of
// "not(val[0])", things like "xor(val[0], val[1])".

// TODO: How does this perform compared to some of the versions above.

var predictDaipp = function(state) {
  // 1. daipp has the option of taking in the parameters of the prior
  // distribution which we don't do here since they are always .5
  // anyway.
  //
  // 2. The position plays the role of address.
  //
  // 3. The final layer that outputs the probability is analogous to
  // the net build by vec2dist.
  //
  // Actually, is this just:
  return predictMlpTanh(state);
};

var updateDaipp = function(state, v) {
  // v is embedded value.
  //
  // 1. daipp embeds the address (position here)
  //
  // 2. Concatenates embedded address, embedded val and context.
  //
  // 3. Generates a new context from that RNN style. i.e linear map +
  // p.w. non-linearity.
  var i = state.i;
  var pos = param({name: 'update_pos' + i});
  var ctx = state.ctx;

  return Math.tanh(
    pos * param({name: 'update_w1'}) +
    v * param({name: 'update_w2'}) +
    ctx * param({name: 'update_w3'}) +
    param({name: 'update_b'}));
};

var daipp = dict({
  predict: predictDaipp,
  update: updateDaipp,
  embed: function(val) {
    return val ? 1 : 0;
  }
});


// TODO: What's the delta between what we're doing here and GRU/LSTM
// for context update?

// TODO: Squishing state. (How does this interact with gates vs. mlp
// for predict?)

// TODO: Initializing update to be the identity helps when we tell the
// model that to put in the context, since this is the "right" answer.
// What's the equivalent of this when there is squishing and the model
// has to learn what to add to the context?


// This specifies the version of the guide we run below.
var len = 5;
var guide = daipp;

var model = function() {
  var steps = iterate(len, initialState, makeStepFn(guide));
  // Condition
  // Exact posterior:
  // [false,true]  : 0.4762870634112165
  // [true,false]  : 0.4762870634112165
  // [false,false] : 0.023712936588783384
  // [true,true]   : 0.023712936588783384
  var firstVal = first(steps).val;
  var lastVal = last(steps).val;
  factor(firstVal !== lastVal ? 0 : -3);
  return [firstVal, lastVal];
};

var params = Optimize(model, {steps: 20000, optMethod: {adam: {stepSize: 0.01}}});
ppp(params);

var m = Infer({method: 'forward', params: params, guide: true, samples: 1000}, model);
display(m.print());
//display(Infer({method: 'enumerate'}, model).print());

// TODO: Generalize this so that I can specify an array of initial
// choices that are fixed.
var fakeit = bindParams(params, function(val) {
  var predict = guide('predict'), update = guide('update'), embed = guide('embed');
  // Pretend we sampled val at the first choice, and then run the
  // guide from there.
  var ctx = update(initialState, embed(val));
  var state = {i: 1, ctx: ctx, val: val, p: null};
  return [state].concat(iterate(len - 1, state, makeStepFn(guide)));
});

ppSteps(fakeit(true));
ppSteps(fakeit(false));

// var ctxrange = map(function(x) { x / 10; }, _.range(10+1));
// var optPredict = bindParams(params, guide('predict'));
// display(map(function(ctx) { return optPredict({i: 1, ctx: ctx}); }, ctxrange));
// display(map(function(ctx) { return optPredict({i: 2, ctx: ctx}); }, ctxrange));
// display(map(function(ctx) { return optPredict({i: 3, ctx: ctx}); }, ctxrange));
// display(map(function(ctx) { return optPredict({i: 4, ctx: ctx}); }, ctxrange));

// var optUpdate = bindParams(params, guide('update'));
// display('i=0');
// display(optUpdate({i: 0, ctx: 0}, guide('embed')(true)));
// display(optUpdate({i: 0, ctx: 0}, guide('embed')(false)));
// display('i=1');
// display(map(function(ctx) { return optUpdate({i: 1, ctx: ctx }, guide('embed')(true)); }, ctxrange));
// display(map(function(ctx) { return optUpdate({i: 1, ctx: ctx }, guide('embed')(false)); }, ctxrange));
// display('i=2');
// display(map(function(ctx) { return optUpdate({i: 2, ctx: ctx }, guide('embed')(true)); }, ctxrange));
// display(map(function(ctx) { return optUpdate({i: 2, ctx: ctx }, guide('embed')(false)); }, ctxrange));
// display('i=3');
// display(map(function(ctx) { return optUpdate({i: 3, ctx: ctx }, guide('embed')(true)); }, ctxrange));
// display(map(function(ctx) { return optUpdate({i: 3, ctx: ctx }, guide('embed')(false)); }, ctxrange));
// display('i=4');
// display(map(function(ctx) { return optUpdate({i: 4, ctx: ctx }, guide('embed')(true)); }, ctxrange));
// display(map(function(ctx) { return optUpdate({i: 4, ctx: ctx }, guide('embed')(false)); }, ctxrange));
