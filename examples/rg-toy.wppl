// iterate(5, 0, function(x) { x + 1 });
// =>
// [1,2,3,4,5]
var iterate = function(n, init, f) {
  if (n === 0) {
    return [];
  } else {
    var val = f(init);
    return [val].concat(iterate(n - 1, val, f));
  }
};

var pp = function(x) { console.log(JSON.stringify(x, null, 2)); };

var isSingleton = function(t) { return t.dims.length === 1 && t.dims[0] === 1; };

// Extract a simple name => value mapping from the webppl params
// object.
var extractParams = function(params) {
  return _.object(map(function(pair) {
    assert.ok(pair[1].length === 1, 'Cannot handle multiple parameters per name.');
    var name = pair[0];
    var val = ad.value(pair[1][0]);
    return [name, isSingleton(val) ? ad.tensor.get(val, 0) : val];
  }, _.pairs(params)));
};

// Pretty print parameters.
var ppp = function(params) { pp(extractParams(params)); };

var bindParams = function(params, f) {
  return function() {
    var args = arguments;
    sample(Infer({method: 'forward', guide: true, params: params}, function() {
      return apply(f, args);
    }));
  };
};

// Make a dictionary type object where look up uses function
// application rather than member expression.
var dict = function(obj) {
  return function(key) {
    return obj[key];
  };
};

// state is threaded along the program and contains:
// i      | the index of the current step
// ctx    | the guide hidden state/context
// val    | sampled value
// p      | parameter of guided Bernoulli
var initialState = {i: 0, ctx: 0, val: undefined, p: undefined};

// This captures the general structure of the recurrent guide for a
// single coin flip. This is the core of the model.
var makeStepFn = function(guide) {
  return function(state) {
    var predict = guide('predict'), update = guide('update'), embed = guide('embed');

    var p = predict(state);

    var val = sample(Bernoulli({p: .5}), {
      guide: Bernoulli({p: p})
    });

    var ctx = update(state, embed(val));

    return {
      i: state.i + 1,
      ctx: ctx,
      val: val,
      p: ad.value(p)
    };
  };
};

// A guide architecture is specified by a function like this. (Written
// in a funny way since we can't call wppl functions using member
// expressions.)
var blah = dict({
  predict: function() {
  },
  update: function() {
  },
  embed: function() {
  }
});

// A mean-field guide.
var mf = dict({
  predict: function(state) {
    return Math.sigmoid(param({name: 'p' + state.i}));
  },
  update: constF(0),
  embed: idF
});

// A simple guide to capture the correlation.

var predictLinSigPerChoice = function(state) {
  // The predict function is a linear function of the context,
  // appropriately squished, with per choice parameters.
  var i = state.i;
  var w = param({name: 'w' + i});
  var b = param({name: 'b' + i});
  return Math.sigmoid(w * state.ctx + b);
};

// Observation: As well as capturing the correlation, this also
// appears to approximate the intermediate choices better then mean
// field.

var simple = dict({
  predict: predictLinSigPerChoice,
  update: function(state, v) {
    // At the first step, stick the (embedded) value into the
    // context. (We know this is sufficient for this problem.)
    return state.i === 0 ? v : state.ctx;
  },
  embed: function(val) {
    return val ? 1 : 0; // Q: How does different embeddings (e.g. .5,-.5) affect optimization?
  }
});

// Now consider allowing the guide the flexibility to alter the
// context after each choice. We don't yet allow sampled values to be
// incorporated into the state. Note that the previous guide is just a
// special case of this, where w=1 and b=0.

var updateLin = function(state, v) {
  var w = param({name: 'w_up'});
  var b = param({name: 'b_up'});
  return state.i === 0 ? v : w * state.ctx + b;
};

// Observation: This change introduces more sensitivity to the choice
// of learning rate. The previous guide usually optimizes OK with a
// step size of 0.1, this usually doesn't.

// Observation: I've not seen this learn that identity for updates.
// Instead, w is always no zero causing the magnitude of the context
// to increase during execution. When w is negative, this sets up an
// oscillation in the context.

var simple2 = dict({
  predict: predictLinSigPerChoice,
  update: updateLin,
  embed: function(val) {
    return val ? 1 : 0;
  }
});

// Now we share weights across choices in the predict function.
// Previously, the predict for each intermediate choice could ignore
// the guide (with w=0) and predict 0.5.

// Note that just adding weight sharing (as in predictLinSig below)
// does not allow posterior to be captured accurately.

var predictLinSig = function(state) {
  var i = state.i;
  var w = param({name: 'w'});
  var b = param({name: 'b'});
  return Math.sigmoid(w * state.ctx + b);
};

// One way you can imagine fixing this is to try to allow the
// intermediate choices to ignore the state.

// (Another, not explored here, might be to have a non-linear update
// function that can approximately count. This might be able to keep
// the state roughly constat for the intermediate choices, and then
// modify it before the final choice. This would still benefit from a
// richer predict function though, as you'd want to map the two
// contexts that encode the inital choice to the same prediction.)

// The question then is, what function will allow the context to be
// ignored base on the current position?

// One option is to have a little 2 layer MLP that takes as input the
// context and an embedding of the current position.

var predictMlp = function(state) {
  var i = state.i;
  var ctx = state.ctx;
  var pos = param({name: 'pos'+i});
  var h1 = Math.sigmoid(ctx * param({name: 'wp11'}) + param({name: 'wp12'}) * pos + param({name: 'bp1'}));
  var h2 = Math.sigmoid(ctx * param({name: 'wp21'}) + param({name: 'wp22'}) * pos + param({name: 'bp2'}));
  return Math.sigmoid(param({name: 'vp1'}) * h1 + param({name: 'vp2'}) * h2 + param({name: 'cp1'}));
};

// (Note that the shallow version of this, sigmoid(pos + w * ctx + b)
// does not work, since pos and ctx can't interact in the required
// way. Which is kinda multiplicatively?)

// This works nicely, though I'm not entirely clear *what* the MLP
// ends up implementing here. My guess is that rather than learning to
// *ignore* the context, it learns to map both of the two contexts it
// encounters to 0.5.

// Observation: This is interesting because it feels like we'd really
// rather learn to ignore the context here. I'm guessing that with the
// MLP all parts of the guide are interconnected in complicated what
// that (1) maybe tricky to learn and (2) might not generalize well.
// If the guide for the intermediate choices could become disconnected
// from the context then we can have independent guide fragments for
// those, and the context part only has to handle capturing the
// correlation.

// With that in mind, we can try using a gate to make the option to
// ignore the context within easy reach of optimization. This is a
// very simple gate: if open we predict from the context, if its
// closed we predict 0.5. This is fine for this model, but would need
// to be generalized for other models.

// Q: How can we keep the guide flexible if it ignores the context. We
// don't really want to have lots of mean field parameters available
// just in case. Perhaps the general version of this to vector valued
// contexts is to allow predict to ignore *parts* of the context.
// Parts of the context might carry information about previous choices
// which can be ignored, while the part of the context encoding the
// current datum is used for a prediction. Having a way that the
// predict function can avoid having to handle variation in irrelevant
// parts of the context might still be useful?

// Q: How does optimization progress compare across mlp vs. gated?

// Observation: A guide optimized with the gated version tends to use
// less extreme values for the context compared to the MLP version.
// The final value of the context might have magnitude ~10 rather than
// ~200.

var predictBasicGate = function(state) {
  var i = state.i;
  var ctx = state.ctx;
  // Per choice gate.
  var pos = param({name: 'pos'+i});
  var gate = Math.sigmoid(pos);
  // Predict from context.
  var w = param({name: 'w'});
  var b = param({name: 'b'});
  return Math.sigmoid(gate * (w * state.ctx + b));
};

var simple3 = dict({
  predict: predictBasicGate,
  //predict: predictMlp,
  update: updateLin,
  embed: function(val) {
    return val ? 1 : 0;
  }
});


// TODO: Squishing state. (How does this interact with gates vs. mlp
// for predict?)

// TODO: Incorporate observations into update state. (Gates here too?)

// TODO: Initializing update to be the identity helps when we tell the
// model that to put in the context, since this is the "right" answer.
// What's the equivalent of this when there is squishing and the model
// has to learn what to add to the context?


// This specifies the version of the guide we run below.
var guide = simple3;

var model = function() {
  var steps = iterate(5, initialState, makeStepFn(guide));
  // Condition
  // Exact posterior:
  // [false,true]  : 0.4762870634112165
  // [true,false]  : 0.4762870634112165
  // [false,false] : 0.023712936588783384
  // [true,true]   : 0.023712936588783384
  var firstVal = first(steps).val;
  var lastVal = last(steps).val;
  factor(firstVal !== lastVal ? 0 : -3);
  return [firstVal, lastVal];
};

var params = Optimize(model, {steps: 20000, optMethod: {adam: {stepSize: 0.01}}});
ppp(params);

var m = Infer({method: 'forward', params: params, guide: true, samples: 1000}, model);
display(m.print());

var fakeit = bindParams(params, function(val) {
  var predict = guide('predict'), update = guide('update'), embed = guide('embed');
  // Pretend we sampled val at the first choice, and then run the
  // guide from there.
  var ctx = update(initialState, embed(val));
  var state = {i: 1, ctx: ctx, val: val, p: null};
  return [state].concat(iterate(4, state, makeStepFn(guide)));
});

var formatStep = function(step) {
  var padNonNeg = function(s) { return s[0] === '-' ? s : ' '.concat(s); };
  var pad = function(n, s) { return s.length >= n ? s : s.concat(' '.repeat(n - s.length)); };
  return [
    'i ', step.i,
    ' | p ', step.p === null ? pad(8, '*') : step.p.toFixed(6),
    ' | val ', pad(5, step.val.toString()),
    ' | ctx ', padNonNeg(ad.value(step.ctx).toFixed(6))
  ].join('');
};

var ppSteps = function(steps) { pp(map(formatStep, steps)); };

ppSteps(fakeit(true));
ppSteps(fakeit(false));


//var f = bindParams(params, predictGated);
//f({i: 3, ctx: 1})
