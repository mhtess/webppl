// // A series of toy problems to test how well guide architectures are
// // able to capture simple patterns of dependency between binary
// // variable.

// // A guide is specified like so:

// var sampleGuide = dict({
//   embed: function() {},
//   predict: function() {},
//   update: function() {}
// });

// // Each task is set in a model where there are n independent binary
// // variables in the prior. Each task induces dependencies between the
// // first 1 or 2 variables samples (which we view as inputs) and the
// // last 1 or 2 variables (which we view as outputs). The guides job is
// // to capture the correct input/output behavior. The intermediary
// // variables serve only to increase the length of the path along which
// // information must be passed.

// // If problems show up optimizing guides here, then it seems unlikely
// // they'll be useful generally?

// // TODO: I should probably have the intermediary variables be
// // independent, but with different probability from the prior so that
// // the guide has to do *something*.

// var sampleTask = function(vals) {
//   assert.ok(vals.length >= 2);
//   // The 'not' task. The guide should learn a noisy logical not. i.e.
//   // input and output should be anti-correlated in the posterior.
//   var input = first(vals);
//   var output = last(vals);
//   factor(input !== output ? 0 : -3);
//   return [input, output]; // The posterior is a marginal on this.
// };

// // TODO: Describe how to see what a guide is computing.

// // To optimize a guide for task:
// var params = opt(task, n, guide);

// // To compare posterior to exact posterior.
// var cmp = compare(task, n, guide, params);
// cmp('kl')() // compute kl divergence
// cmp('posteriors')() // print full posteriors

// // To inspect the computation performed by the guide for a given
// // 'input'.
// inspect(input, task, n, guide, params);

// ======================================================================
// Tasks
// ----------------------------------------------------------------------

var notTask = function(vals) {
  assert.ok(vals.length >= 2);
  var input = first(vals);
  var output = last(vals);
  factor(input !== output ? 0 : -3);
  return [input, output];
};

// ======================================================================

var dict = function(obj) {
  return function(key) {
    return obj[key];
  };
};

var boolToInt = function(b) { return b ? 1 : 0; };
var boolToVec = function(b) { return Vector([boolToInt(b)]); };

// ======================================================================
// Guides
// ----------------------------------------------------------------------

var mf = dict({
  initialCtx: zeros([1, 1]),
  embed: idF,
  predict: function(state) {
    var i = state.i;
    return Math.sigmoid(param({name: 'p' + i}));
  },
  update: constF(zeros([1, 1]))
});

// ----------------------------------------------------------------------

// This is similar to the guide in the webppl-daipp package.

// Adds the trailing dimension to a rank 1 tensor.
var tovec = function(t) {
  var dims = ad.value(t).dims;
  assert.ok(dims.length === 1);
  return T.reshape(t, [dims[0], 1]);
};

// Unwraps the value in a vector of length 1.
var toscalar = function(t) {
  assert.ok(_.isEqual(ad.value(t).dims, [1, 1]));
  return T.get(t, 0);
};

var concat = function() {
  return tovec(T.concat(arguments));
};

var isInt = function(i) {
  return _.isNumber(i) && Math.floor(i) === i;
};

var isPosInt = function(i) {
  return isInt(i) && i > 0;
};

var predictMlp = function(state, args) {
  assert.ok(args.nonlinearfn, 'Non-linear function not given.');
  var nonlinearfn = args.nonlinearfn;
  var i = state.i;
  var ctx = state.ctx;
  // Note: Position embedding uses same dim as context.
  var pos = param({name: 'pos' + i, dims: [args.ctxdim, 1]});
  var w1 = param({name: 'w1', dims: [args.hdim, 2 * args.ctxdim]});
  var b1 = param({name: 'b1', dims: [args.hdim, 1]});
  var w2 = param({name: 'w2', dims: [1, args.hdim]});
  var b2 = param({name: 'b2', dims: [1, 1]});
  var x = concat(ctx, pos);
  var h = nonlinearfn(T.add(T.dot(w1, x), b1));
  var o = T.sigmoid(T.add(T.dot(w2, h), b2));
  return toscalar(o);
};

var sigmoid = function(x) { return T.sigmoid(x); };
var tanh = function(x) { return T.tanh(x); };

var daippUpdate = function(state, embededVal, args) {
  // daipp does the following:
  // 1. embeds the address (position here)
  // 2. Concatenates embedded address, embedded val and context.
  // 3. Generates a new context from that RNN style. i.e linear map +
  // p.w. non-linearity.
  var i = state.i;
  var ctx = state.ctx;
  var pos = param({name: 'pos' + i, dims: [args.ctxdim, 1]});

  var w = param({name: 'w', dims: [args.ctxdim, 2 * args.ctxdim + 1]});
  var b = param({name: 'b', dims: [args.ctxdim, 1]});

  var x = concat(ctx, pos, embededVal);
  return T.tanh(T.add(T.dot(w, x), b));
};

var daipp = function(a) {
  var args = util.mergeDefaults(a, {
    nonlinearfn: tanh,
    ctxdim: 1,
    hdim: 2
  });
  assert.ok(isPosInt(args.ctxdim));
  assert.ok(isPosInt(args.hdim));
  return dict({
    initialCtx: zeros([args.ctxdim, 1]),
    embed: boolToVec,
    predict: function(state) {
      return predictMlp(state, args);
    },
    update: function(state, embededVal) {
      return daippUpdate(state, embededVal, args);
    }
  });
};

// ----------------------------------------------------------------------

// Simple gated predict and update functions.

// TODO: Fix that this can only predict .5 if the gate is closed.
var predictSimpleGate = function(state) {
  var i = state.i;
  var ctx = T.get(state.ctx, 0);
  // Per choice gate.
  var pos = param({name: 'sg_pos' + i});
  var gate = Math.sigmoid(pos);
  // Predict from context.
  var w = param({name: 'sg_w'});
  var b = param({name: 'sg_b'});
  return Math.sigmoid(gate * (w * ctx + b));
};

var updateSimpleGate = function(state, embededVal) {
  var i = state.i;
  var ctx = T.get(state.ctx, 0);
  var g = Math.sigmoid(param({name: 'sg_update' + i}));
  // note, always in {0,1} when embed fn is boolToInt.
  return Vector([g * embededVal + (1 - g) * ctx]);
};

var simpleGated = dict({
  initialCtx: zeros([1, 1]),
  embed: boolToInt,
  predict: predictSimpleGate,
  update: updateSimpleGate
});

// ======================================================================

var pp = function(obj) {
  return display(JSON.stringify(obj, null, 2));
};

var formatStep = function(step) {
  var padNonNeg = function(s) { return s[0] === '-' ? s : ' '.concat(s); };
  var pad = function(n, s) { return s.length >= n ? s : s.concat(' '.repeat(n - s.length)); };
  return [
    'i ', step.i,
    ' | p ', step.p === null ? pad(6, '*') : step.p.toFixed(4),
    ' | val ', pad(5, step.val.toString()),
    ' | ctx ', map(function(x) { padNonNeg(x.toFixed(4)); }, step.ctx.toFlatArray())
  ].join('');
};

var ppSteps = function(steps) { pp(map(formatStep, steps)); };

var bindParams = function(params, f) {
  return function() {
    var args = arguments;
    sample(Infer({method: 'forward', guide: true, params: params}, function() {
      return ad.valueRec(apply(f, args));
    }));
  };
};

var iterate = function(n, init, f) {
  if (n === 0) {
    return [];
  } else {
    var val = f(init);
    return [val].concat(iterate(n - 1, val, f));
  }
};

var step = function(guide, state, maybeVal) {
  var embed = guide('embed'),
      predict = guide('predict'),
      update = guide('update');

  var p = predict(state);

  var val = (maybeVal !== undefined) ?
      maybeVal :
      sample(Bernoulli({p: .5}), {guide: Bernoulli({p: p})});

  var ctx = update(state, embed(val));

  return {i: state.i + 1, ctx: ctx, val: val, p: p};
};

var prior = function(task, n, guide, maybeVals) {
  var vals = maybeVals !== undefined ? maybeVals : [];
  var initialState = {i: 0, ctx: guide('initialCtx')};
  return iterate(n, initialState, function(state) {
    return step(guide, state, vals[state.i]);
  });
};

var model = function(task, n, guide) {
  var steps = prior(task, n, guide);
  return task(_.pluck(steps, 'val'));
};

var opt = function(task, n, guide) {
  return Optimize(function() {
    return model(task, n, guide);
  }, {
    steps: 10000,
    optMethod: {adam: {stepSize: .01}}
  });
};

var compare = function(task, n, guide, params) {
  var p = Infer({
    method: 'enumerate'
  }, function() {
    model(task, n, guide);
  });
  var q = Infer({
    samples: 1000,
    method: 'forward',
    guide: true,
    params: params
  }, function() {
    model(task, n, guide);
  });
  return dict({
    kl: function() {
      return kl(q,p);
    },
    posteriors: function() {
      return [
        'Exact:',
        p.print(),
        'Approx:',
        q.print()
      ].join('\n');
    }
  });
};

var inspect = function(input, task, n, guide, params) {
  var run = bindParams(params, prior);
  return run(task, n, guide, input);
};

var ppinspect = function() { ppSteps(apply(inspect, arguments)); };

var runTest = function(task, n, guide) {
  var params = opt(task, n, guide);
  var cmp = compare(task, n, guide, params);
  var kl = cmp('kl')();
  display(cmp('posteriors')());
  display('kl: ' + kl);
  ppinspect([true], task, n, guide, params);
  ppinspect([false], task, n, guide, params);
  return kl;
};

//runTest(notTask, 5, mf);
//runTest(notTask, 5, daipp({ctxdim: 1, hdim: 2}));
//runTest(notTask, 5, daipp(sigmoid));
runTest(notTask, 5, simpleGated);
