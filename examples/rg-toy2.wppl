// // A series of toy problems to test how well guide architectures are
// // able to capture simple patterns of dependency between binary
// // variable.

// // A guide is specified like so:

// var sampleGuide = dict({
//   embed: function() {},
//   predict: function() {},
//   update: function() {}
// });

// // Each task is set in a model where there are n independent binary
// // variables in the prior. Each task induces dependencies between the
// // first 1 or 2 variables samples (which we view as inputs) and the
// // last 1 or 2 variables (which we view as outputs). The guides job is
// // to capture the correct input/output behavior. The intermediary
// // variables serve only to increase the length of the path along which
// // information must be passed.

// // If problems show up optimizing guides here, then it seems unlikely
// // they'll be useful generally?

// // TODO: I should probably have the intermediary variables be
// // independent, but with different probability from the prior so that
// // the guide has to do *something*.

// var sampleTask = function(vals) {
//   assert.ok(vals.length >= 2);
//   // The 'not' task. The guide should learn a noisy logical not. i.e.
//   // input and output should be anti-correlated in the posterior.
//   var input = first(vals);
//   var output = last(vals);
//   factor(input !== output ? 0 : -3);
//   return [input, output]; // The posterior is a marginal on this.
// };

// // TODO: Describe how to see what a guide is computing.

// // To optimize a guide for task:
// var params = opt(task, n, guide);

// // To compare posterior to exact posterior.
// var cmp = compare(task, n, guide, params);
// cmp('kl')() // compute kl divergence
// cmp('posteriors')() // print full posteriors

// // To inspect the computation performed by the guide for a given
// // 'input'.
// inspect(input, task, n, guide, params);

// ======================================================================
// Tasks
// ----------------------------------------------------------------------

var notTask = function(vals) {
  assert.ok(vals.length >= 2);
  var input = first(vals);
  var output = last(vals);
  factor(input !== output ? 0 : -3);
  return [input, output];
};

// ======================================================================

var dict = function(obj) {
  return function(key) {
    return obj[key];
  };
};

var boolToInt = function(b) { return b ? 1 : 0; };
var boolToVec = function(b) { return Vector([boolToInt(b)]); };

// ======================================================================
// Guides
// ----------------------------------------------------------------------

var mf = dict({
  embed: idF,
  predict: function(state) {
    var i = state.i;
    return Math.sigmoid(param({name: 'p' + i}));
  },
  update: constF(0)
});

// ----------------------------------------------------------------------

// This is similar to the guide in the webppl-daipp package.

// Adds the trailing dimension to a rank 1 tensor.
var tovec = function(t) {
  var dims = ad.value(t).dims;
  assert.ok(dims.length === 1);
  return T.reshape(t, [dims[0], 1]);
};

// Unwraps the value in a vector of length 1.
var toscalar = function(t) {
  assert.ok(_.isEqual(ad.value(t).dims, [1, 1]));
  return T.get(t, 0);
};

var concat = function() {
  return tovec(T.concat(arguments));
};

var isInt = function(i) {
  return _.isNumber(i) && Math.floor(i) === i;
};

var isPosInt = function(i) {
  return isInt(i) && i > 0;
};

var predictMlp = function(state, args) {
  assert.ok(args.nonlinearfn, 'Non-linear function not given.');
  var nonlinearfn = args.nonlinearfn;
  var i = state.i;
  var ctx = state.ctx;
  // Note: Position embedding uses same dim as context.
  var pos = param({name: 'pos' + i, dims: [args.ctxdim, 1]});
  var w1 = param({name: 'w1', dims: [args.hdim, 2 * args.ctxdim]});
  var b1 = param({name: 'b1', dims: [args.hdim, 1]});
  var w2 = param({name: 'w2', dims: [1, args.hdim]});
  var b2 = param({name: 'b2'});
  var x = concat(ctx, pos);
  var h = nonlinearfn(T.add(T.dot(w1, x), b2));
  var o = T.sigmoid(T.add(T.dot(w2, h), b2));
  return toscalar(o);
};

var sigmoid = function(x) { return T.sigmoid(x); };
var tanh = function(x) { return T.tanh(x); };

var daippUpdate = function(state, embededVal, args) {
  // daipp does the following:
  // 1. embeds the address (position here)
  // 2. Concatenates embedded address, embedded val and context.
  // 3. Generates a new context from that RNN style. i.e linear map +
  // p.w. non-linearity.
  var i = state.i;
  var ctx = state.ctx;
  // TODO: Use same embedding as predict.
  var pos = param({name: 'daipp_update_pos' + i, dims: [args.ctxdim, 1]});

  var w = param({name: 'w', dims: [args.ctxdim, 2 * args.ctxdim + 1]});
  var b = param({name: 'b', dims: [args.ctxdim, 1]});

  var x = concat(ctx, pos, embededVal);
  return T.tanh(T.add(T.dot(w, x), b));
};

var daipp = function(a) {
  var args = util.mergeDefaults(a, {
    nonlinearfn: tanh,
    ctxdim: 1,
    hdim: 2
  });
  assert.ok(isPosInt(args.ctxdim));
  assert.ok(isPosInt(args.hdim));
  return dict({
    initialCtx: zeros([args.ctxdim, 1]),
    embed: boolToVec,
    predict: function(state) {
      return predictMlp(state, args);
    },
    update: function(state, embededVal) {
      return daippUpdate(state, embededVal, args);
    }
  });
};

// ----------------------------------------------------------------------

// Simple gated predict and update functions.

// TODO: Fix that this can only predict .5 if the gate is closed.
var predictSimpleGate = function(state) {
  var i = state.i;
  var ctx = state.ctx;
  // Per choice gate.
  var pos = param({name: 'sg_pos' + i});
  var gate = Math.sigmoid(pos);
  // Predict from context.
  var w = param({name: 'sg_w'});
  var b = param({name: 'sg_b'});
  return Math.sigmoid(gate * (w * state.ctx + b));
};

var updateSimpleGate = function(state, embededVal) {
  var i = state.i;
  var g = Math.sigmoid(param({name: 'sg_update' + i}));
  // note, always in {0,1} when embed fn is boolToInt.
  return g * embededVal + (1 - g) * state.ctx;
};

var simpleGated = dict({
  embed: boolToInt,
  predict: predictSimpleGate,
  update: updateSimpleGate
});

// ======================================================================

var pp = function(obj) {
  return display(JSON.stringify(obj, null, 2));
};

var formatStep = function(step) {
  var padNonNeg = function(s) { return s[0] === '-' ? s : ' '.concat(s); };
  var pad = function(n, s) { return s.length >= n ? s : s.concat(' '.repeat(n - s.length)); };
  return [
    'i ', step.i,
    ' | p ', step.p === null ? pad(6, '*') : step.p.toFixed(4),
    ' | val ', pad(5, step.val.toString()),
    ' | ctx ', map(function(x) { padNonNeg(x.toFixed(4)); }, step.ctx.toFlatArray())
  ].join('');
};

var ppSteps = function(steps) { pp(map(formatStep, steps)); };

var bindParams = function(params, f) {
  return function() {
    var args = arguments;
    sample(Infer({method: 'forward', guide: true, params: params}, function() {
      return ad.valueRec(apply(f, args));
    }));
  };
};

var iterate = function(n, init, f) {
  if (n === 0) {
    return [];
  } else {
    var val = f(init);
    return [val].concat(iterate(n - 1, val, f));
  }
};

var step = function(guide, state, maybeVal) {
  var embed = guide('embed'),
      predict = guide('predict'),
      update = guide('update');

  var p = predict(state);

  var val = (maybeVal !== undefined) ?
      maybeVal :
      sample(Bernoulli({p: .5}), {guide: Bernoulli({p: p})});

  var ctx = update(state, embed(val));

  return {i: state.i + 1, ctx: ctx, val: val, p: p};
};

var prior = function(task, n, guide, maybeVals) {
  var vals = maybeVals !== undefined ? maybeVals : [];
  var initialState = {i: 0, ctx: guide('initialCtx')};
  return iterate(n, initialState, function(state) {
    return step(guide, state, vals[state.i]);
  });
};

var model = function(task, n, guide) {
  var steps = prior(task, n, guide);
  return task(_.pluck(steps, 'val'));
};

var opt = function(task, n, guide) {
  return Optimize(function() {
    return model(task, n, guide);
  }, {
    steps: 10000,
    optMethod: {adam: {stepSize: .01}}
  });
};

var compare = function(task, n, guide, params) {
  var p = Infer({
    method: 'enumerate'
  }, function() {
    model(task, n, guide);
  });
  var q = Infer({
    samples: 1000,
    method: 'forward',
    guide: true,
    params: params
  }, function() {
    model(task, n, guide);
  });
  return dict({
    kl: function() {
      return kl(q,p);
    },
    posteriors: function() {
      return [
        'Exact:',
        p.print(),
        'Approx:',
        q.print()
      ].join('\n');
    }
  });
};

var inspect = function(input, task, n, guide, params) {
  var run = bindParams(params, prior);
  return run(task, n, guide, input);
};

var ppinspect = function() { ppSteps(apply(inspect, arguments)); };

var runTest = function(task, n, guide) {
  var params = opt(task, n, guide);
  var cmp = compare(task, n, guide, params);
  var kl = cmp('kl')();
  display(cmp('posteriors')());
  display('kl: ' + kl);
  ppinspect([true], task, n, guide, params);
  ppinspect([false], task, n, guide, params);
  return kl;
};

//runTest(notTask, 5, mf);
runTest(notTask, 5, daipp({ctxdim: 1, hdim: 2}));
//runTest(notTask, 5, daipp(sigmoid));
//runTest(notTask, 5, simpleGated);



// the distributions of the kl for the old and new implementations
// don't look quite the same?

// new

// [ 0.00005048958920646999,
//   0.00041530905160734426,
//   0.00047304740331023665,
//   0.000740674655762448,
//   0.0010669527988934392,
//   0.0012113385839963783,
//   0.0015464395387218728,
//   0.0016329357278533786,
//   0.012279969642383308,
//   0.05182903569913041,
//   0.06306663871006533,
//   0.09656650365788955,
//   0.1161504602418092,
//   0.11930942662205199,
//   0.14432498076280226,
//   0.41596886076083905,
//   0.6151824111436953,
//   0.6221397157343032,
//   0.641454461980192,
//   0.7963687127740993 ]

// old

// [ 0.00014168504970950985,
//   0.00034132112402776756,
//   0.0004094388154753624,
//   0.0004611634578768776,
//   0.0007456179430760934,
//   0.001066093617076159,
//   0.0011152458520369696,
//   0.0018901027116461234,
//   0.0021566821337582957,
//   0.0029759681175389003,
//   0.003921953602609527,
//   0.11913920449582475,
//   0.6126467053664953,
//   0.6148620936279653,
//   0.6177978030055581,
//   0.6186068010878708,
//   0.6231257621870042,
//   0.6357888633466553,
//   0.799095473915135,
//   0.8770959381380576 ]


// new

// [ 0.00017889772837419333,
//   0.00020038955976845428,
//   0.00022964118266783143,
//   0.0003770609585632555,
//   0.0004065112380437219,
//   0.0004595035903821945,
//   0.0006160804299616303,
//   0.0007723154117848068,
//   0.0008631143294430748,
//   0.0011228025135604745,
//   0.00124478738250133,
//   0.0012554488403043497,
//   0.001975176044508845,
//   0.0023650924072481545,
//   0.0023994522041189854,
//   0.0028680248951938776,
//   0.002975968117538901,
//   0.0030807982272607268,
//   0.0031347072558527214,
//   0.003298929651812916,
//   0.003700867403597763,
//   0.05704041804365825,
//   0.057180409837579875,
//   0.06415659145934506,
//   0.07503866894214675,
//   0.08657722621815864,
//   0.0886869421786998,
//   0.10404899071078143,
//   0.1122675682469781,
//   0.11886668876142588,
//   0.12706329599892252,
//   0.1548800413354822,
//   0.3408743825298785,
//   0.3757183417024831,
//   0.5773422820657963,
//   0.6148410382772187,
//   0.6196453367454057,
//   0.6224288385187616,
//   0.627029941614281,
//   0.6275244819062318,
//   0.6287390937078591,
//   0.6287692929903924,
//   0.6292004224013743,
//   0.6313177995554515,
//   0.636036103517573,
//   0.6376100593270505,
//   0.6406748208824733,
//   0.666912549350499,
//   0.6863825984439745,
//   0.9160515889616173 ]

// // old

// [ 0.00016428664040964158,
//   0.0002271583088365222,
//   0.0002664850655784805,
//   0.00032878187974508906,
//   0.00038697597340898223,
//   0.0004065112380437219,
//   0.0004247186533339123,
//   0.00046557816029645267,
//   0.0005458711984947101,
//   0.0007660990513905213,
//   0.000782726271360893,
//   0.0007923745833874549,
//   0.0008011607279531574,
//   0.0009479981382956477,
//   0.001012088623634973,
//   0.0011430754294499378,
//   0.0012696606381803338,
//   0.0012956997747039442,
//   0.0013028852452947569,
//   0.001309496680123052,
//   0.0014505693191365302,
//   0.0015326573070614316,
//   0.001699170454956031,
//   0.0018682354341603216,
//   0.0018754170584479313,
//   0.0019314875645988997,
//   0.001979340114537022,
//   0.0020039072089561988,
//   0.002112444029210559,
//   0.0022586879242099712,
//   0.0030494480265969917,
//   0.003916066837615306,
//   0.004467805619862616,
//   0.004743751085569751,
//   0.6039121542406097,
//   0.6076351646567302,
//   0.6094457749321665,
//   0.6108716165012092,
//   0.6181240293209239,
//   0.6191478151925172,
//   0.6212626940987331,
//   0.6293649797130907,
//   0.6316705086598124,
//   0.6580403322782702,
//   0.7690066707868861,
//   0.7831887335505734,
//   0.7898771303070823,
//   0.8468269955669449,
//   0.8586179100407454,
//   0.9017820384506703 ]
