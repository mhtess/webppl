// A series of toy problems to test how well guide architectures are
// able to capture simple patterns of dependency between binary
// variables.

// A guide is specified like so:

// var sampleGuide = dict({
//   embed: function() {},
//   predict: function() {},
//   update: function() {}
// });

// Each task is set in a model where there are n independent binary
// variables in the prior. Each task induces dependencies between the
// first 1 or 2 variables samples (which we view as inputs) and the
// last 1 or 2 variables (which we view as outputs). The guides job is
// to capture the correct input/output behavior. The intermediary
// variables serve only to increase the length of the path along which
// information must be passed.

// If problems show up optimizing guides here, then it seems unlikely
// they'll be useful generally?

// TODO: I should probably have the intermediary variables be
// independent, but with different probabilities from the prior so
// that the guide has to do *something*.

// var sampleTask = function(vals) {
//   assert.ok(vals.length >= 2);
//   // The 'not' task. The guide should learn a noisy logical not. i.e.
//   // input and output should be anti-correlated in the posterior.
//   var input = first(vals);
//   var output = last(vals);
//   factor(input !== output ? 0 : -3);
//   return [input, output]; // The posterior is a marginal on this.
// };

// // To optimize a guide for task:
// var params = opt(task, n, guide);

// // To compare posterior to exact posterior.
// var cmp = compare(task, n, guide, params);
// cmp('kl')() // compute kl divergence
// cmp('posteriors')() // print full posteriors

// // To inspect the computation performed by the guide for a given
// // 'input'.
// inspect(input, task, n, guide, params);

// ======================================================================
// Tasks
// ----------------------------------------------------------------------

var notTask = function(vals) {
  assert.ok(vals.length >= 2);
  var input = first(vals);
  var output = last(vals);
  factor(input !== output ? 0 : -3);
  return [input, output];
};

var binaryLogicTask = function(f) {
  return function(vals) {
    assert.ok(vals.length >= 3);
    var input0 = vals[0];
    var input1 = vals[1];
    var output = last(vals);
    factor(f(input0, input1) === output ? 0 : -3);
    return [input0, input1, output];
  };
};

var orTask = binaryLogicTask(function(x, y) { return x || y; });
var andTask = binaryLogicTask(function(x, y) { return x && y; });
var xorTask = binaryLogicTask(function(x, y) { return x !== y; });
var impTask = binaryLogicTask(function(x, y) { return !x || y; });

var reverse = function(arr) {
  if (arr.length === 0) {
    return [];
  } else {
    return reverse(arr.slice(1)).concat(arr[0]);
  }
};

var reverseTask = function(n) {
  return function(vals) {
    assert.ok(vals.length >= 2 * n);
    var input = vals.slice(0, n);
    var output = vals.slice(-n);
    factor(_.isEqual(reverse(input), output) ? 0 : -3);
    return input.concat(output);
  };
};

var countTask = function(vals) {
  assert.ok(vals.length >= 5);
  var input = vals.slice(0, 3);
  var output = vals.slice(-2);
  var actual = reduce(function(x, acc) { return acc + (x ? 1 : 0); }, 0, input);
  var computed = sum(mapIndexed(function(i, x) { return Math.pow(2, i) * (x ? 1 : 0); }, reverse(output)));
  factor(actual === computed ? 0 : -3);
  return [actual, computed];
};

// ======================================================================

var dict = function(obj) {
  return function(key) {
    return obj[key];
  };
};

var boolToInt = function(b) { return b ? 1 : 0; };
var boolToVec = function(b) { return Vector([boolToInt(b)]); };

// Adds the trailing dimension to a rank 1 tensor.
var tovec = function(t) {
  var dims = ad.value(t).dims;
  assert.ok(dims.length === 1);
  return T.reshape(t, [dims[0], 1]);
};

// Unwraps the value in a vector of length 1.
var toscalar = function(t) {
  assert.ok(_.isEqual(ad.value(t).dims, [1, 1]));
  return T.get(t, 0);
};

var concat = function() {
  return tovec(T.concat(arguments));
};

var isInt = function(i) {
  return _.isNumber(i) && Math.floor(i) === i;
};

var isPosInt = function(i) {
  return isInt(i) && i > 0;
};

var sigmoid = function(x) { return T.sigmoid(x); };
var tanh = function(x) { return T.tanh(x); };

// ======================================================================
// Guides
// ----------------------------------------------------------------------

var mf = dict({
  initialCtx: zeros([1, 1]),
  embed: idF,
  predict: function(state) {
    var i = state.i;
    return Math.sigmoid(param({name: 'p' + i}));
  },
  update: constF(zeros([1, 1]))
});

// ----------------------------------------------------------------------

// This is similar to the guide in the webppl-daipp package.

var predictmlp = function(state, args) {
  assert.ok(args.nonlin, 'Non-linear function not given.');
  var nonlin = args.nonlin;
  var i = state.i;
  var ctx = state.ctx;
  var pos = param({name: 'pos' + i, dims: [args.posdim, 1]});
  var w1 = param({name: 'w1', dims: [args.hdim, args.posdim + args.ctxdim]});
  var b1 = param({name: 'b1', dims: [args.hdim, 1]});
  var w2 = param({name: 'w2', dims: [1, args.hdim]});
  var b2 = param({name: 'b2', dims: [1, 1]});
  var x = concat(ctx, pos);
  var h = nonlin(T.add(T.dot(w1, x), b1));
  var o = T.sigmoid(T.add(T.dot(w2, h), b2));
  return toscalar(o);
};

var updaternn = function(state, embededVal, args) {
  // daipp does the following:
  // 1. embeds the address (position here)
  // 2. Concatenates embedded address, embedded val and context.
  // 3. Generates a new context from that RNN style. i.e linear map +
  // p.w. non-linearity.
  var i = state.i;
  var ctx = state.ctx;
  var pos = param({name: 'pos' + i, dims: [args.posdim, 1]});

  var w = param({name: 'w', dims: [args.ctxdim, 2 * args.ctxdim + 1]});
  var b = param({name: 'b', dims: [args.ctxdim, 1]});

  var x = concat(ctx, pos, embededVal);
  return T.tanh(T.add(T.dot(w, x), b));
};

// TODO: Try iRNN?

var rnnGuide = function(a) {
  var args = util.mergeDefaults(a, {
    nonlin: tanh,
    ctxdim: 1,
    posdim: 1,
    hdim: 2
  });
  assert.ok(isPosInt(args.ctxdim));
  assert.ok(isPosInt(args.posdim));
  assert.ok(isPosInt(args.hdim));
  return dict({
    initialCtx: zeros([args.ctxdim, 1]),
    embed: boolToVec,
    predict: function(state) {
      return predictmlp(state, args);
    },
    update: function(state, embededVal) {
      return updaternn(state, embededVal, args);
    }
  });
};

// ----------------------------------------------------------------------

// Simple gated predict and update functions.

// TODO: Fix that this can only predict .5 if the gate is closed.
var predictSimpleGate = function(state) {
  var i = state.i;
  var ctx = T.get(state.ctx, 0);
  // Per choice gate.
  var pos = param({name: 'pos' + i});
  var gate = Math.sigmoid(pos);
  // Predict from context.
  var w = param({name: 'sg_w'});
  var b = param({name: 'sg_b'});
  return Math.sigmoid(gate * (w * ctx + b));
};

var updateSimpleGate = function(state, embededVal) {
  var i = state.i;
  var ctx = T.get(state.ctx, 0);
  var g = Math.sigmoid(param({name: 'pos' + i}));
  // note, always in {0,1} when embed fn is boolToInt.
  return Vector([g * embededVal + (1 - g) * ctx]);
};

var simpleGated = dict({
  initialCtx: zeros([1, 1]),
  embed: boolToInt,
  predict: predictSimpleGate,
  update: updateSimpleGate
});

// ----------------------------------------------------------------------

// The variant described in "Empirical Evaluation of Gated Recurrent
// Neural Networks on Sequence Modeling" which computes the candidate
// activation in a slightly different way from the original paper.

var updategru = function(state, embeddedVal, args) {
  var i = state.i;
  var ctx = state.ctx;
  var ctxdim = args.ctxdim;
  var posdim = args.posdim;
  // The input is the concatenation of the value sampled and the
  // current position. (Both embedded.)
  var pos = param({name: 'pos' + i, dims: [posdim, 1]});
  var x = concat(embeddedVal, pos);
  var xdim = posdim + 1;

  // Reset gate.
  var Wr = param({name: 'Wr', dims: [ctxdim, xdim]});
  var Ur = param({name: 'Ur', dims: [ctxdim, ctxdim]});
  var r = T.sigmoid(T.add(T.dot(Wr, x), T.dot(Ur, ctx)));

  // Candidate activation.
  var W = param({name: 'W', dims: [ctxdim, xdim]});
  var U = param({name: 'U', dims: [ctxdim, ctxdim]});
  var candidate = T.tanh(T.add(T.dot(W, x), T.dot(U, T.mul(r, ctx))));

  // Update gate.
  var Wz = param({name: 'Wz', dims: [ctxdim, xdim]});
  var Uz = param({name: 'Uz', dims: [ctxdim, ctxdim]});
  var z = T.sigmoid(T.add(T.dot(Wz, x), T.dot(Uz, ctx)));

  // Output is interpolation between old ctx and candidate.
  var oneminusz = T.add(T.neg(z), 1);
  var out = T.add(T.mul(oneminusz, ctx), T.mul(z, candidate));

  return out;
};

var gruGuide = function(a) {
  var args = util.mergeDefaults(a, {
    posdim: 1,
    ctxdim: 1,
    hdim: 2,
    nonlin: tanh
  });
  assert.ok(isPosInt(args.posdim));
  assert.ok(isPosInt(args.ctxdim));
  assert.ok(isPosInt(args.hdim));
  return dict({
    initialCtx: zeros([args.ctxdim, 1]),
    embed: boolToVec,
    predict: function(state) {
      // TODO: Is there a gated analog to update?
      return predictmlp(state, args);
    },
    update: function(state, embeddedVal) {
      return updategru(state, embeddedVal, args);
    }
  });
};

// ----------------------------------------------------------------------

// This follows "Generating sequences with recurrent neural networks"
// (Graves 2013) via "Empirical Evaluation of Gated Recurrent Neural
// Networks on Sequence Modeling".

var updatelstm = function(state, embeddedVal, args) {
  var ctx = state.ctx;
  var initsd = args.initsd;

  // For the LSTM the context vector is a concatenation of the cell
  // state and the hidden state: ctx = concat(h, c). The cell state
  // and hidden state are the same size in the LSTM.
  var ctxdim = args.ctxdim;
  var cdim = args.ctxdim / 2;
  var hprev = T.range(ctx, 0, cdim);
  var cprev = T.range(ctx, cdim, ctxdim);

  // Position embedding.
  var posdim = args.posdim;
  var pos = param({name: 'pos' + state.i, dims: [posdim, 1]});

  // The input to the LSTM is the concatenation of the value sampled
  // and the current position. (Both embedded.) To simplify what
  // follows we further concatenate this input and the previous hidden
  // state in to a single vector, hprevx.
  // var x = concat(embeddedVal, pos);
  // var hprevx = concat(hprev, x);
  var hprevx = concat(hprev, embeddedVal, pos);
  var hprevxdim = cdim + 1 + posdim;

  // Forget gate.
  var Wf = param({name: 'Wf', dims: [cdim, hprevxdim], sigma: initsd});
  var f = T.sigmoid(T.dot(Wf, hprevx));

  // Input gate.
  var Wi = param({name: 'Wi', dims: [cdim, hprevxdim], sigma: initsd});
  var i = T.sigmoid(T.dot(Wi, hprevx));

  // Candidate cell state.
  var Wc = param({name: 'Wc', dims: [cdim, hprevxdim], sigma: initsd});
  var candidate = T.tanh(T.dot(Wc, hprevx));

  // New cell state.
  var c = T.add(T.mul(cprev, f), T.mul(candidate, i));

  // Output gate.
  var Wo = param({name: 'Wo', dims: [cdim, hprevxdim], sigma: initsd});
  var o = T.sigmoid(T.dot(Wo, hprevx));

  // New hidden state.
  var h = T.mul(o, T.tanh(c));

  return concat(h, c);
};

// TODO: Refactor. This is very similar to predictMlp. The difference
// is that we only put the first half of the context (the LSTM hidden
// state) through the MLP.

var predictlstm = function(state, args) {
  assert.ok(args.nonlin, 'Non-linear function not given.');
  var nonlin = args.nonlin;
  var i = state.i;
  var ctx = state.ctx;
  var cdim = args.ctxdim / 2;
  var lstmhid = T.range(ctx, 0, cdim);
  var posdim = args.posdim;
  var pos = param({name: 'pos' + state.i, dims: [posdim, 1]});

  var w1 = param({name: 'w1', dims: [args.hdim, cdim + posdim]});
  var b1 = param({name: 'b1', dims: [args.hdim, 1]});
  var w2 = param({name: 'w2', dims: [1, args.hdim]});
  var b2 = param({name: 'b2', dims: [1, 1]});

  var x = concat(lstmhid, pos);
  var h = nonlin(T.add(T.dot(w1, x), b1));
  var o = T.sigmoid(T.add(T.dot(w2, h), b2));
  return toscalar(o);
};

var lstmGuide = function(a) {
  var args = util.mergeDefaults(a, {
    posdim: 1,
    ctxdim: 2,
    hdim: 2,
    nonlin: tanh,
    initsd: .1
  });
  assert.ok(isPosInt(args.posdim));
  assert.ok(isPosInt(args.hdim));
  assert.ok(isInt(args.ctxdim));
  assert.ok(args.ctxdim >= 2);
  assert.ok(args.ctxdim % 2 === 0);
  return dict({
    initialCtx: zeros([args.ctxdim, 1]),
    embed: boolToVec,
    predict: function(state) {
      return predictlstm(state, args);
    },
    update: function(state, embeddedVal) {
      return updatelstm(state, embeddedVal, args);
    }
  });
};

// ======================================================================

var pp = function(obj) {
  return display(JSON.stringify(obj, null, 2));
};

var formatStep = function(step) {
  var padNonNeg = function(s) { return s[0] === '-' ? s : ' '.concat(s); };
  var pad = function(n, s) { return s.length >= n ? s : s.concat(' '.repeat(n - s.length)); };
  return [
    'i ', step.i,
    ' | p ', step.p === null ? pad(6, '*') : step.p.toFixed(4),
    ' | val ', pad(5, step.val.toString()),
    ' | ctx ', map(function(x) { padNonNeg(x.toFixed(4)); }, step.ctx.toFlatArray())
  ].join('');
};

var ppSteps = function(steps) { pp(map(formatStep, steps)); };

var bindParams = function(params, f) {
  return function() {
    var args = arguments;
    sample(Infer({method: 'forward', guide: true, params: params}, function() {
      return ad.valueRec(apply(f, args));
    }));
  };
};

var iterate = function(n, init, f) {
  if (n === 0) {
    return [];
  } else {
    var val = f(init);
    return [val].concat(iterate(n - 1, val, f));
  }
};

var step = function(guide, state, maybeVal) {
  var embed = guide('embed'),
      predict = guide('predict'),
      update = guide('update');

  var p = predict(state);

  var val = (maybeVal !== undefined) ?
      maybeVal :
      sample(Bernoulli({p: .5}), {guide: Bernoulli({p: p})});

  var ctx = update(state, embed(val));

  return {i: state.i + 1, ctx: ctx, val: val, p: p};
};

var prior = function(task, n, guide, maybeVals) {
  var vals = maybeVals !== undefined ? maybeVals : [];
  var initialState = {i: 0, ctx: guide('initialCtx')};
  return iterate(n, initialState, function(state) {
    return step(guide, state, vals[state.i]);
  });
};

var model = function(task, n, guide) {
  var steps = prior(task, n, guide);
  return task(_.pluck(steps, 'val'));
};

var opt = function(task, n, guide) {
  return Optimize(function() {
    return model(task, n, guide);
  }, {
    steps: 10000,
    optMethod: {adam: {stepSize: .01}}
  });
};

var compare = function(task, n, guide, params) {
  var p = Infer({
    method: 'enumerate'
  }, function() {
    model(task, n, guide);
  });
  var q = Infer({
    samples: 1000,
    method: 'forward',
    guide: true,
    params: params
  }, function() {
    model(task, n, guide);
  });
  return dict({
    kl: function() {
      return kl(q,p);
    },
    posteriors: function() {
      return [
        'Exact:',
        p.print(),
        'Approx:',
        q.print()
      ].join('\n');
    }
  });
};

var inspect = function(input, task, n, guide, params) {
  var run = bindParams(params, prior);
  return run(task, n, guide, input);
};

var ppinspect = function() { ppSteps(apply(inspect, arguments)); };

var runTest = function(task, n, guide) {
  var params = opt(task, n, guide);
  var cmp = compare(task, n, guide, params);
  var kl = cmp('kl')();
  display(cmp('posteriors')());
  display('kl: ' + kl);
  ppinspect([true], task, n, guide, params);
  ppinspect([false], task, n, guide, params);
  return kl;
};

//runTest(notTask, 5, mf);
//runTest(notTask, 5, daipp({ctxdim: 1, hdim: 2}));
//runTest(notTask, 5, daipp(sigmoid));
runTest(notTask, 5, simpleGated);
