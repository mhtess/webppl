// var graph = JSON.parse(fs.read('examples/qmr-graph.json'));

var graph = {
  diseases: [{p: .5}, {p: .5}],
  symptoms: [
    {leakProb: 0.001, parents: [{i: 0, p: .9}, {i: 1, p: .1}]},
    {leakProb: 0.001, parents: [{i: 0, p: .9}, {i: 1, p: .1}]}
  ]
};

//console.log(JSON.stringify(graph, null, 2));

var noisyOrProb = function(node, diseases) {
  var pFalse = (1 - node.leakProb) * product(map(function(parent) {
    return diseases[parent.i] ? (1 - parent.p) : 1;
  }, node.parents));
  return 1 - pFalse;
};

// TODO: Thought - it's unclear to me how to evaluate this in a way
// that will distinguish between mean field and a richer guide.
// (Comparing "true" latents with samples from optimized guide doesn't
// see like it will work. If there are two explanations in the true
// posterior, mean-field will sometimes find the right one because of
// mode seeking, and the structure posterior will onyly sometimes find
// it because it might sample from either mode.) As an alternative,
// can we compute the probability the guide assigns to the "true"
// latents by exploiting the sequentiality? Predict, check score of
// "true" val, pretend "true" val was samples, predict, etc.? The
// problem then is, one of the problems with optimizing the ELBo is
// over confidence, which complicates this comparison.


var _model = function() {
  var diseases = map(function(node) {
    return sample(Bernoulli({p: node.p}));
  }, graph.diseases);

  var symptoms = map2(function(node, val) {
    observe(Bernoulli({p: noisyOrProb(node, diseases)}), val);
    //return sample(Bernoulli({p: noisyOrProb(node, diseases)}));
  }, graph.symptoms, [false, true]);

  return {
    diseases: diseases,
    //symptoms: symptoms
  };
};

var iterate = function(n, init, f) {
  if (n === 0) {
    return [];
  } else {
    var val = f(init);
    return [val].concat(iterate(n - 1, val, f));
  }
};

var dict = function(obj) {
  return function(key) {
    return obj[key];
  };
};

var predictMlp = function(state, nonlinearfn) {
  assert.ok(nonlinearfn, 'Non-linear function not given.');
  var i = state.i;
  var ctx = state.ctx;
  var pos = param({name: 'pos'+i});
  var h1 = nonlinearfn(ctx * param({name: 'wp11'}) + param({name: 'wp12'}) * pos + param({name: 'bp1'}));
  var h2 = nonlinearfn(ctx * param({name: 'wp21'}) + param({name: 'wp22'}) * pos + param({name: 'bp2'}));
  return Math.sigmoid(param({name: 'vp1'}) * h1 + param({name: 'vp2'}) * h2 + param({name: 'cp1'}));
};

var sigmoid = function(x) { return Math.sigmoid(x); };
var tanh = function(x) { return Math.tanh(x); };

var predictMlpSigmoid = function(state) { return predictMlp(state, sigmoid); };
var predictMlpTanh = function(state) { return predictMlp(state, tanh); };

var predictDaipp = function(state) {
  return predictMlpTanh(state);
};

var updateDaipp = function(state, v) {
  var i = state.i;
  var pos = param({name: 'update_pos' + i});
  var ctx = state.ctx;

  return Math.tanh(
    pos * param({name: 'update_w1'}) +
    v * param({name: 'update_w2'}) +
    ctx * param({name: 'update_w3'}) +
    param({name: 'update_b'}));
};

var daipp = dict({
  predict: predictDaipp,
  update: updateDaipp,
  embed: function(val) {
    return val ? 1 : 0;
  },
  init: function(obs) {
    //return 0;
    // obs are embedded observations.
    assert.ok(obs.length === 2, 'init assumes there are 2 observations.');
    var o1 = obs[0];
    var o2 = obs[1];
    var w1 = param({name: 'init_w1'});
    var w2 = param({name: 'init_w2'});
    var b = param({name: 'init_b'});
    return Math.tanh(o1 * w1 + o2 * w2 + b);
  }
});

var mf = dict({
  predict: function(state) {
    var i = state.i;
    return Math.sigmoid(param({name: 'p' + i}));
  },
  update: function(state, v) {
    return state.ctx;
  },
  embed: function(val) {
    return val ? 1 : 0;
  },
  init: function() {
    return 0;
  }
});

//var guide = mf;
var guide = daipp;
var predict = guide('predict'),
    update = guide('update'),
    embed = guide('embed'),
    init = guide('init');

var model = function(observations) {

  var steps = iterate(
    graph.diseases.length,
    {i: 0, ctx: init(map(embed, observations))}, // Initial state.
    function(state) {
      var p = predict(state);

      var val = sample(Bernoulli({p: graph.diseases[state.i].p}), {
        guide: Bernoulli({p: p})
      });

      var ctx = update(state, embed(val));

      return {
        i: state.i + 1,
        ctx: ctx,
        val: val
        // p: p
      };
    });

  var diseases = _.pluck(steps, 'val');
  var symptoms = map2(function(node, val) {
    observe(Bernoulli({p: noisyOrProb(node, diseases)}), val);
  }, graph.symptoms, observations);

  return {diseases: diseases};
};

// Optimize the guide on a subset of possible observations.
var params = Optimize(function() {
  model([false, true]);
  model([true, false]);
  model([false, false]);
}, {steps: 10000, optMethod: {adam: {stepSize: 0.01}}});

var klToExactPosterior = function(observations, params) {
  var q = Infer({
    method: 'forward',
    samples: 1000,
    guide: true,
    params: params
  }, function() {
    return model(observations).diseases;
  });
  var p = Infer({method: 'enumerate'}, function() {
    return model(observations).diseases;
  });
  return kl(q,p);
};

display(map(function(obs) {
  return {
    kl: klToExactPosterior(obs, params),
    obs: obs
  };
}, [
  [false, true],
  [true, false],
  [false, false],
  [true, true] // Not optimized for.
]));
