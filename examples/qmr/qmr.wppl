// var graph = {
//   diseases: [{p: .5}, {p: .5}],
//   symptoms: [
//     {leakProb: 0.001, parents: [{i: 0, p: .9}, {i: 1, p: .1}]},
//     {leakProb: 0.001, parents: [{i: 0, p: .9}, {i: 1, p: .1}]}
//   ]
// };

// Requires webppl-fs package.
var graph = JSON.parse(fs.read('examples/qmr/10x10.json'));

//console.log(JSON.stringify(graph, null, 2));

var noisyOrProb = function(node, diseases) {
  var pFalse = (1 - node.leakProb) * product(map(function(parent) {
    return diseases[parent.i] ? (1 - parent.p) : 1;
  }, node.parents));
  return 1 - pFalse;
};

var iterate = function(n, init, f) {
  if (n === 0) {
    return [];
  } else {
    var val = f(init);
    return [val].concat(iterate(n - 1, val, f));
  }
};

var dict = function(obj) {
  return function(key) {
    return obj[key];
  };
};

var predictMlp = function(state, nonlinearfn) {
  assert.ok(nonlinearfn, 'Non-linear function not given.');
  var i = state.i;
  var ctx = state.ctx;
  var pos = param({name: 'pos'+i});
  var h1 = nonlinearfn(ctx * param({name: 'wp11'}) + param({name: 'wp12'}) * pos + param({name: 'bp1'}));
  var h2 = nonlinearfn(ctx * param({name: 'wp21'}) + param({name: 'wp22'}) * pos + param({name: 'bp2'}));
  return Math.sigmoid(param({name: 'vp1'}) * h1 + param({name: 'vp2'}) * h2 + param({name: 'cp1'}));
};

var sigmoid = function(x) { return Math.sigmoid(x); };
var tanh = function(x) { return Math.tanh(x); };

var predictMlpSigmoid = function(state) { return predictMlp(state, sigmoid); };
var predictMlpTanh = function(state) { return predictMlp(state, tanh); };

var predictDaipp = function(state) {
  return predictMlpTanh(state);
};

var updateDaipp = function(state, v) {
  var i = state.i;
  var pos = param({name: 'update_pos' + i});
  var ctx = state.ctx;

  return Math.tanh(
    pos * param({name: 'update_w1'}) +
    v * param({name: 'update_w2'}) +
    ctx * param({name: 'update_w3'}) +
    param({name: 'update_b'}));
};

var daipp = dict({
  init: function(obs) {
    //return 0;
    // obs are embedded observations.
    assert.ok(obs.length === 2, 'init assumes there are 2 observations.');
    var o1 = obs[0];
    var o2 = obs[1];
    var w1 = param({name: 'init_w1'});
    var w2 = param({name: 'init_w2'});
    var b = param({name: 'init_b'});
    return Math.tanh(o1 * w1 + o2 * w2 + b);
  },
  predict: predictDaipp,
  update: updateDaipp,
  embed: function(val) {
    return val ? 1 : 0;
  }
});

var mf = dict({
  init: function() {
    return 0;
  },
  predict: function(state) {
    var i = state.i;
    return Math.sigmoid(param({name: 'p' + i}));
  },
  update: function(state, v) {
    return state.ctx;
  },
  embed: function(val) {
    return val ? 1 : 0;
  }
});

//var guide = mf;
var guide = daipp;
var predict = guide('predict'),
    update = guide('update'),
    embed = guide('embed'),
    init = guide('init');

var sampleLatents = function(observations) {
  var steps = iterate(
    graph.diseases.length,
    {i: 0, ctx: init(map(embed, observations))}, // Initial state.
    function(state) {
      var p = predict(state);

      var val = sample(Bernoulli({p: graph.diseases[state.i].p}), {
        guide: Bernoulli({p: p})
      });

      var ctx = update(state, embed(val));

      return {i: state.i + 1, ctx: ctx, val: val};
    });

  var diseases = _.pluck(steps, 'val');
  return diseases;
};

var model = function(observations) {
  assert.ok(observations.length === 0 ||
            observations.length === graph.symptoms.length);
  var diseases = sampleLatents(observations);
  var symptoms = map2(function(node, val) {
    observe(Bernoulli({p: noisyOrProb(node, diseases)}), val);
  }, graph.symptoms, observations);
  return {diseases: diseases, symptoms: symptoms};
};

var klToExactPosterior = function(observations, params, n) {
  // TODO: It would be nice to not have to sample here. A global
  // parameter store make enumerating guides easier I think.
  var q = Infer({
    method: 'forward',
    samples: n,
    guide: true,
    params: params
  }, function() {
    return model(observations).diseases;
  });
  var p = Infer({method: 'enumerate'}, function() {
    return model(observations).diseases;
  });
  return kl(q,p);
};

var indicesWhereTrue = function(arr) {
  return filter(function(i) {
    return i >= 0;
  }, mapIndexed(function(i, x) {
    return x ? i : -1;
  }, arr));
};

// **************************************************

// Uses the 10x10 graph.

var obs = repeat(9, falseF).concat(true);

var params = Optimize(function() {
  model(obs);
}, {steps: 10000, optMethod: {adam: {stepSize: 0.01}}});

// The 10 most probable configurations of the latents (showing the
// indices of latents set to true) from the true posterior are:

// [6]   : 0.309387091638377
// [9]   : 0.2427410042844149
// []    : 0.17942893122248812
// [5]   : 0.08695470509940276
// [3]   : 0.06173520610128723
// [6,9] : 0.01983404311986202
// [7]   : 0.012722005258345694
// [2,6] : 0.00930207069644313
// [3,6] : 0.008623134532906822

// With this observation the posterior has the property that it's
// likely to be caused by either this or that, or be a false alarm.

// A case where there are 2 observed symptoms (i.e. obs[3], obs[4] in
// the case i tried) also has a similar structure.

display(klToExactPosterior(obs, params, 10000));

Infer({method: 'forward', samples: 10000, guide: true, params: params}, function() {
  return indicesWhereTrue(model(obs).diseases);
});

// mean field
// kl 0.2662361693680813
// []      : 0.4014          overly confident the patient is well...
// [6]     : 0.2096
// [9]     : 0.1167
// [6,9]   : 0.0588
// [5]     : 0.0424
// [3]     : 0.031
// [5,6]   : 0.0184
// [2]     : 0.0148
// [3,6]   : 0.014
// [5,9]   : 0.0108
// [3,9]   : 0.0096
// [2,6]   : 0.0076
// [3,6,9] : 0.0054
// [5,6,9] : 0.0054
// [7]     : 0.005

// daipp (no init. as we only have one observation)
// kl 0.05529032700329285
// [6]     : 0.3233
// [9]     : 0.2541
// []      : 0.1547
// [5]     : 0.0969
// [3]     : 0.0752
// [6,9]   : 0.0277
// [2]     : 0.0131
// [7]     : 0.013
// [5,9]   : 0.0066
// [3,9]   : 0.0058
// [8]     : 0.0046
// [0]     : 0.0045
// [5,6]   : 0.0037
// [3,6]   : 0.0021
// [4]     : 0.002

// Q: Is this guide able to achieve kl=0 if given enough time?

// Q: Try optimizing for some subset of observations and seeing if the
// guide generalizes. Using samples from the model might not be a good
// idea since (as Andreas pointed out) these will often be all false.
// Consider all observations where a single symptom, or pairs of
// symptoms are observed, or subsets of those?

// **************************************************

// Q: When exactly are we trying to evaluate? The ability of the guide
// to capture correlations, how well the guide generalized to
// observations not directly optimized for? How quickly we can do
// inference for a new observation starting from a guide optimized for
// other observations?

// Q: How do we evaluate this when we can no longer compute the exact
// posterior?

// It would be sensible to evaluate any candidate metric on small
// problems so we can check that it is in fact a good proxy for kl or
// whatever we're interested in.

// Comparing "true" latents with samples from the optimized guide
// doesn't seem like it would distinguish between mean
// field/structured guide. Imagine the case where there are two
// equally good explanations/modes in the true posterior. Mean field
// might get the latents right about half the time (when it locks onto
// the "correct" mode) and the recurrent guide might get it right
// about half the time when it samples from the correct mode.

// We might try to compute the score of the "true" latents under the
// approximate posterior, but the problem here is that if optimization
// has locked onto the "correct" mode, then it will over estimate the
// score relative to the true posterior. So higher score isn't better
// here.

// **************************************************

// Optimize the guide on a subset of possible observations.
// var params = Optimize(function() {
//   model([false, true]);
//   model([true, false]);
//   model([false, false]);
// }, {steps: 10000, optMethod: {adam: {stepSize: 0.01}}});

// display(map(function(obs) {
//   return {
//     kl: klToExactPosterior(obs, params, 1000),
//     obs: obs
//   };
// }, [
//   [false, true],
//   [true, false],
//   [false, false],
//   [true, true] // Not optimized for.
// ]));

// Before optimization there is distance between the guide and true
// posterior for all possible observations.

// [ { kl: 0.9037423466514625, obs: [ false, true ] },
//   { kl: 0.9205977261711624, obs: [ true, false ] },
//   { kl: 1.4619992171834526, obs: [ false, false ] },
//   { kl: 4.188901291858791, obs: [ true, true ] } ]

// After optimization, the guide is very close to the true posterior
// for the observations optimized for. It's also closer than it was to
// the true posterior for an observation *not* directly optimized for.

// [ { kl: 0.0010352020561353679, obs: [ false, true ] },
//   { kl: 0.0009561009665033128, obs: [ true, false ] },
//   { kl: 0.0012572534343594875, obs: [ false, false ] },
//   { kl: 1.1999742812475505, obs: [ true, true ] } ]
