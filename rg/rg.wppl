// ======================================================================

var dict = function(obj) {
  return function(key) {
    return obj[key];
  };
};

var boolToInt = function(b) { return b ? 1 : 0; };
var boolToVec = function(b) { return Vector([boolToInt(b)]); };

// Adds the trailing dimension to a rank 1 tensor.
var tovec = function(t) {
  var dims = ad.value(t).dims;
  assert.ok(dims.length === 1);
  return T.reshape(t, [dims[0], 1]);
};

// Unwraps the value in a vector of length 1.
var toscalar = function(t) {
  assert.ok(_.isEqual(ad.value(t).dims, [1, 1]));
  return T.get(t, 0);
};

var concat = function() {
  return tovec(T.concat(arguments));
};

var isInt = function(i) {
  return _.isNumber(i) && Math.floor(i) === i;
};

var isPosInt = function(i) {
  return isInt(i) && i > 0;
};

var sigmoid = function(x) { return T.sigmoid(x); };
var tanh = function(x) { return T.tanh(x); };
var relu = function(x) { return nn.relu.eval(x); };

var softplus = function(x) {
  return Math.log(1 + Math.exp(x));
};

var dims = function(x) { return ad.value(x).dims; };

var linear = function(name, o, initw) {
  return function(x) {
    var i = dims(x)[0];
    var w = param({name: name + 'w', dims: [o, i], init: initw});
    var b = param({name: name + 'b', dims: [o, 1], sigma: 0});
    return T.add(T.dot(w, x), b);
  };
};

var linearLayerNorm = function(name, o, initw) {
  return function(x) {
    var i = dims(x)[0];
    var g = param({name: name + 'g', dims: [o, 1], mu: 1, sigma: 0}); // gain
    var w = param({name: name + 'w', dims: [o, i], init: initw});
    var b = param({name: name + 'b', dims: [o, 1], sigma: 0});
    var a = T.dot(w, x);
    var mu = T.sumreduce(a) / o;
    var acentered = T.sub(a, mu);
    var sd = Math.sqrt(T.sumreduce(T.pow(acentered, 2)) / o);
    return T.add(T.mul(T.div(g, sd), acentered), b);
  };
};

var linearWeightNorm = function(name, o, initv) {
  return function(x) {
    var i = dims(x)[0];
    var g = T.exp(param({name: name + 'g', dims: [o, 1], sigma: 0})); // gain
    var v = param({name: name + 'v', dims: [o, i], init: initv});
    var norm = T.sqrt(T.sumreduce0(T.pow(v, 2)));
    var b = param({name: name + 'b', dims: [o, 1], sigma: 0});
    return T.add(T.mul(T.dot(v, x), T.div(g, norm)), b);
  };
};

// ======================================================================
// Guides
// ----------------------------------------------------------------------

var mfGuide = function(args) {
  assert.ok(isPosInt(args.posdim));
  assert.ok(_.isFunction(args.embed));
  assert.ok(_.isFunction(args.adapt));
  return dict({
    initialCtx: null,
    embed: args.embed,
    predict: function(state) {
      var i = state.i;
      // A vector of mean field parameters.
      var pos = param({name: 'pos' + i, dims: [args.posdim, 1]});
      var adapt = args.adapt;
      return adapt(pos);
    },
    update: constF(null)
  });
};

// ----------------------------------------------------------------------

// This is similar to the guide in the webppl-daipp package.

// TODO: Check that bad weight init. of mlp isn't hurting.

var predictmlp = function(state, args) {
  assert.ok(args.nonlin, 'Non-linear function not given.');
  var nonlin = args.nonlin;
  var i = state.i;
  var ctx = state.ctx;
  var hdim = args.hdim;
  var pos = param({name: 'pos' + i, dims: [args.posdim, 1]});
  var lin =
        args.norm === 'none' ? linear('predict', hdim, args.init) :
        args.norm === 'layer' ? linearLayerNorm('predict', hdim, args.init) :
        args.norm === 'weight' ? linearWeightNorm('predict', hdim, args.init) :
        null;
  assert.ok(lin);
  var x = concat(ctx, pos);
  var net = compose(nonlin, lin);
  var h = net(x);
  var adapt = args.adapt;
  return adapt(h, args);
};

var updaternn = function(state, embededVal, args) {
  // daipp does the following:
  // 1. embeds the address (position here)
  // 2. Concatenates embedded address, embedded val and context.
  // 3. Generates a new context from that RNN style. i.e linear map +
  // p.w. non-linearity.
  var i = state.i;
  var ctx = state.ctx;
  var ctxdim = args.ctxdim;
  var pos = param({name: 'pos' + i, dims: [args.posdim, 1]});
  var name = 'update' + (args.tieWeights ? '' : i);
  var lin =
        args.norm === 'none' ? linear(name, ctxdim, args.init) :
        args.norm === 'layer' ? linearLayerNorm(name, ctxdim, args.init) :
        args.norm === 'weight' ? linearWeightNorm(name, ctxdim, args.init) :
        null;
  assert.ok(lin);
  var net = compose(tanh, lin);
  var x = concat(ctx, pos, embededVal);
  return net(x);
};


var rnnGuide = function(a) {
  var args = util.mergeDefaults(a, {
    nonlin: tanh,
    ctxdim: 1,
    posdim: 1,
    hdim: 2,
    norm: 'none',
    init: 'rand',
    tieWeights: true
  });
  assert.ok(isPosInt(args.ctxdim));
  assert.ok(isPosInt(args.posdim));
  assert.ok(isPosInt(args.hdim));
  assert.ok(_.isFunction(args.embed));
  assert.ok(_.isFunction(args.adapt));
  assert.ok(_.includes(['none', 'layer', 'weight'], args.norm));
  assert.ok(_.isBoolean(args.tieWeights));
  return dict({
    initialCtx: zeros([args.ctxdim, 1]),
    embed: args.embed,
    predict: function(state) {
      return predictmlp(state, args);
    },
    update: function(state, embededVal) {
      return updaternn(state, embededVal, args);
    }
  });
};


// ----------------------------------------------------------------------

var updateirnn = function(state, embededVal, args) {
  var i = state.i;
  var ctx = state.ctx;
  var posdim = args.posdim;
  var ctxdim = args.ctxdim;

  var pos = param({name: 'pos' + i, dims: [posdim, 1]});
  var v = param({name: 'v', dims: [ctxdim, ctxdim], init: 'id'});
  var w = param({name: 'w', dims: [ctxdim, posdim + 1]});
  var b = param({name: 'b', dims: [ctxdim, 1]});

  var x = concat(pos, embededVal);
  return relu(T.add(T.add(T.dot(v, ctx), T.dot(w, x)), b));
};

var irnnGuide = function(a) {
  var args = util.mergeDefaults(a, {
    nonlin: tanh,
    ctxdim: 1,
    posdim: 1,
    hdim: 2,
    norm: 'none'
  });
  assert.ok(isPosInt(args.ctxdim));
  assert.ok(isPosInt(args.posdim));
  assert.ok(isPosInt(args.hdim));
  assert.ok(_.isFunction(args.embed));
  assert.ok(_.isFunction(args.adapt));
  assert.ok(_.includes(['none', 'layer', 'weight'], args.norm));
  return dict({
    initialCtx: zeros([args.ctxdim, 1]),
    embed: args.embed,
    predict: function(state) {
      return predictmlp(state, args);
    },
    update: function(state, embededVal) {
      return updateirnn(state, embededVal, args);
    }
  });
};

// ----------------------------------------------------------------------

// The variant described in "Empirical Evaluation of Gated Recurrent
// Neural Networks on Sequence Modeling" which computes the candidate
// activation in a slightly different way from the original paper.

var updategru = function(state, embeddedVal, args) {
  var i = state.i;
  var ctx = state.ctx;
  var ctxdim = args.ctxdim;
  var posdim = args.posdim;
  // The input is the concatenation of the value sampled and the
  // current position. (Both embedded.)
  var pos = param({name: 'pos' + i, dims: [posdim, 1]});
  var x = concat(embeddedVal, pos);
  var xdim = posdim + 1;

  // Reset gate.
  var Wr = param({name: 'Wr', dims: [ctxdim, xdim]});
  var Ur = param({name: 'Ur', dims: [ctxdim, ctxdim]});
  var r = T.sigmoid(T.add(T.dot(Wr, x), T.dot(Ur, ctx)));

  // Candidate activation.
  var W = param({name: 'W', dims: [ctxdim, xdim]});
  var U = param({name: 'U', dims: [ctxdim, ctxdim]});
  var candidate = T.tanh(T.add(T.dot(W, x), T.dot(U, T.mul(r, ctx))));

  // Update gate.
  var Wz = param({name: 'Wz', dims: [ctxdim, xdim]});
  var Uz = param({name: 'Uz', dims: [ctxdim, ctxdim]});
  var z = T.sigmoid(T.add(T.dot(Wz, x), T.dot(Uz, ctx)));

  // Output is interpolation between old ctx and candidate.
  var oneminusz = T.add(T.neg(z), 1);
  var out = T.add(T.mul(oneminusz, ctx), T.mul(z, candidate));

  return out;
};


var gruGuide = function(a) {
  var args = util.mergeDefaults(a, {
    posdim: 1,
    ctxdim: 1,
    hdim: 2,
    nonlin: tanh,
    norm: 'none'
  });
  assert.ok(isPosInt(args.posdim));
  assert.ok(isPosInt(args.ctxdim));
  assert.ok(isPosInt(args.hdim));
  assert.ok(_.isFunction(args.embed));
  assert.ok(_.isFunction(args.adapt));
  assert.ok(_.includes(['none', 'layer', 'weight'], args.norm));
  return dict({
    initialCtx: zeros([args.ctxdim, 1]),
    embed: args.embed,
    predict: function(state) {
      // TODO: Is there a gated analog to update?
      return predictmlp(state, args);
    },
    update: function(state, embeddedVal) {
      return updategru(state, embeddedVal, args);
    }
  });
};

// ----------------------------------------------------------------------

// This follows "Generating sequences with recurrent neural networks"
// (Graves 2013) via "Empirical Evaluation of Gated Recurrent Neural
// Networks on Sequence Modeling". (When peephole connections are
// enabled.)

var updatelstm = function(state, embeddedVal, args) {
  var ctx = state.ctx;
  var initsd = args.initsd;

  // For the LSTM the context vector is a concatenation of the cell
  // state and the hidden state: ctx = concat(h, c). The cell state
  // and hidden state are the same size in the LSTM.
  var ctxdim = args.ctxdim;
  var cdim = args.ctxdim / 2;
  var hprev = T.range(ctx, 0, cdim);
  var cprev = T.range(ctx, cdim, ctxdim);

  // Position embedding.
  var posdim = args.posdim;
  var pos = param({name: 'pos' + state.i, dims: [posdim, 1]});

  // The input to the LSTM is the concatenation of the value sampled
  // and the current position. (Both embedded.) To simplify what
  // follows we further concatenate this input and the previous hidden
  // state in to a single vector, hprevx.
  // var x = concat(embeddedVal, pos);
  // var hprevx = concat(hprev, x);
  var hprevx = concat(hprev, embeddedVal, pos);
  var hprevxdim = cdim + 1 + posdim;

  // Forget gate.
  var Wf = param({name: 'Wf', dims: [cdim, hprevxdim], sigma: initsd});
  // Init. bias to 1 so that we don't forget things initially. Not
  // original source, but described in "An Empirical Exploration of
  // Recurrent Network Architectures".
  var bf = param({name: 'bf', dims: [cdim, 1], mu: 1, sigma: 0});
  var fpeep = args.peep ?
      T.mul(param({name: 'Vf', dims: [cdim, 1]}), cprev) :
      zeros([cdim, 1]);
  var f = T.sigmoid(T.add(T.add(T.dot(Wf, hprevx), fpeep), bf));

  // Input gate.
  var Wi = param({name: 'Wi', dims: [cdim, hprevxdim], sigma: initsd});
  var bi = param({name: 'bi', dims: [cdim, 1], mu: 0, sigma: 0});
  var ipeep = args.peep ?
      T.mul(param({name: 'Vi', dims: [cdim, 1]}), cprev) :
      zeros([cdim, 1]);
  var i = T.sigmoid(T.add(T.add(T.dot(Wi, hprevx), ipeep), bi));

  // Candidate cell state.
  var Wc = param({name: 'Wc', dims: [cdim, hprevxdim], sigma: initsd});
  var bc = param({name: 'bc', dims: [cdim, 1], mu: 0, sigma: 0});
  var candidate = T.tanh(T.add(T.dot(Wc, hprevx), bc));

  // New cell state.
  var c = T.add(T.mul(cprev, f), T.mul(candidate, i));

  // Output gate.
  var Wo = param({name: 'Wo', dims: [cdim, hprevxdim], sigma: initsd});
  var bo = param({name: 'bo', dims: [cdim, 1], mu: 0, sigma: 0});
  var opeep = args.peep ?
      T.mul(param({name: 'Vo', dims: [cdim, 1]}), cprev) :
      zeros([cdim, 1]);
  var o = T.sigmoid(T.add(T.add(T.dot(Wo, hprevx), opeep), bo));

  // New hidden state.
  var h = T.mul(o, T.tanh(c));

  return concat(h, c);
};

// TODO: Refactor. This is very similar to predictMlp. The difference
// is that we only put the first half of the context (the LSTM hidden
// state) through the MLP.

// I wonder whether it would be nice to use webppl functions for
// building nets rather than adnn. The problem that adnn solves (it's
// a pain passing parameters around) isn't one we have in webppl,
// since we have `param`. We go to a lot of trouble to make use of the
// adnn solution, but perhaps we achieve much the same thing with just
// a few lines of code?

var predictlstm = function(state, args) {
  assert.ok(args.nonlin, 'Non-linear function not given.');
  var nonlin = args.nonlin;
  var ctx = state.ctx;
  var cdim = args.ctxdim / 2;
  var lstmhid = T.range(ctx, 0, cdim);
  var posdim = args.posdim;
  var pos = param({name: 'pos' + state.i, dims: [posdim, 1]});

  var w1 = param({name: 'w1', dims: [args.hdim, cdim + posdim]});
  var b1 = param({name: 'b1', dims: [args.hdim, 1]});
  //var w2 = param({name: 'w2', dims: [1, args.hdim]});
  //var b2 = param({name: 'b2', dims: [1, 1]});

  var x = concat(lstmhid, pos);
  var h = nonlin(T.add(T.dot(w1, x), b1));
  //var o = T.add(T.dot(w2, h), b2);
  //return toscalar(o);
  var adapt = args.adapt;
  return adapt(h, args);
};

var lstmGuide = function(a) {
  var args = util.mergeDefaults(a, {
    posdim: 1,
    ctxdim: 2,
    hdim: 2,
    nonlin: tanh,
    initsd: .1,
    peep: false, // Add peephole connections to gates.
    norm: 'none'
  });
  assert.ok(isPosInt(args.posdim));
  assert.ok(isPosInt(args.hdim));
  assert.ok(isInt(args.ctxdim));
  assert.ok(args.ctxdim >= 2);
  assert.ok(args.ctxdim % 2 === 0);
  assert.ok(_.isFunction(args.embed));
  assert.ok(_.isFunction(args.adapt));
  assert.ok(_.includes(['none', 'layer', 'weight'], args.norm));
  return dict({
    initialCtx: zeros([args.ctxdim, 1]),
    embed: args.embed,
    predict: function(state) {
      return predictlstm(state, args);
    },
    update: function(state, embeddedVal) {
      return updatelstm(state, embeddedVal, args);
    }
  });
};
